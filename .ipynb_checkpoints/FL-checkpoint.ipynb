{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26e68a91-1c1a-45d7-883e-177047fce802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 1 Coefficients: [ 1.54702547e-04 -1.22302337e-03  7.64345295e-01  8.48115199e-02\n",
      "  4.41670814e-01  3.53623120e-01  5.37683894e-04  5.90079238e-05]\n",
      "Client 1 Intercept: 5.774058109752646e-05\n",
      "Client 2 Coefficients: [ 1.54702547e-04 -1.22302337e-03  7.64345295e-01  8.48115199e-02\n",
      "  4.41670814e-01  3.53623120e-01  5.37683894e-04  5.90079238e-05]\n",
      "Client 2 Intercept: 5.774058109752646e-05\n",
      "Client 3 Coefficients: [ 1.54702547e-04 -1.22302337e-03  7.64345295e-01  8.48115199e-02\n",
      "  4.41670814e-01  3.53623120e-01  5.37683894e-04  5.90079238e-05]\n",
      "Client 3 Intercept: 5.774058109752646e-05\n",
      "Client 4 Coefficients: [ 3.53241478e-04 -8.53621509e-04  7.66118385e-01  8.47888754e-02\n",
      "  4.41010276e-01  3.52858297e-01 -4.85743316e-04  1.15542825e-03]\n",
      "Client 4 Intercept: 0.0005938679868076994\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Load the models\n",
    "model_client_1 = joblib.load('model_client_1.pkl')\n",
    "model_client_2 = joblib.load('model_client_2.pkl')\n",
    "model_client_3 = joblib.load('model_client_3.pkl')\n",
    "model_client_4 = joblib.load('model_client_4.pkl')\n",
    "\n",
    "# Extract weights (coefficients)\n",
    "coef_1 = model_client_1.coef_\n",
    "coef_2 = model_client_2.coef_\n",
    "coef_3 = model_client_3.coef_\n",
    "coef_4 = model_client_4.coef_\n",
    "\n",
    "# Extract biases (intercepts)\n",
    "intercept_1 = model_client_1.intercept_\n",
    "intercept_2 = model_client_2.intercept_\n",
    "intercept_3 = model_client_3.intercept_\n",
    "intercept_4 = model_client_4.intercept_\n",
    "\n",
    "# Display results\n",
    "print(\"Client 1 Coefficients:\", coef_1)\n",
    "print(\"Client 1 Intercept:\", intercept_1)\n",
    "\n",
    "print(\"Client 2 Coefficients:\", coef_2)\n",
    "print(\"Client 2 Intercept:\", intercept_2)\n",
    "\n",
    "print(\"Client 3 Coefficients:\", coef_3)\n",
    "print(\"Client 3 Intercept:\", intercept_3)\n",
    "\n",
    "print(\"Client 4 Coefficients:\", coef_4)\n",
    "print(\"Client 4 Intercept:\", intercept_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3d9013b-cbbe-4af7-83b5-49efb3953ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Coefficients: [ 2.04337280e-04 -1.13067290e-03  7.64788567e-01  8.48058588e-02\n",
      "  4.41505680e-01  3.53431914e-01  2.81827092e-04  3.33113006e-04]\n",
      "Global Intercept: 0.0001917724325250697\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Aggregate coefficients (average)\n",
    "global_coef = np.mean([coef_1, coef_2, coef_3, coef_4], axis=0)\n",
    "\n",
    "# Aggregate intercepts (average)\n",
    "global_intercept = np.mean([intercept_1, intercept_2, intercept_3, intercept_4])\n",
    "\n",
    "print(\"Global Coefficients:\", global_coef)\n",
    "print(\"Global Intercept:\", global_intercept)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b160c4c-1a1e-4383-9662-52497c3bd6cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['global_model.pkl']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump((global_coef, global_intercept), 'global_model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2efc9527-4a13-4c44-8678-b47214c002ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter Soil Type (Sandy, Clay, Loam, Silt, Peaty, Chalky):  Sandy\n",
      "Enter Crop (Cotton, Rice, Barley, Soybean, Wheat, Maize):  Rice\n",
      "Enter Rainfall in mm:  576\n",
      "Enter Temperature in Celsius:  45\n",
      "Fertilizer Used (Yes, No):  Yes\n",
      "Irrigation Used (Yes, No):  Yes\n",
      "Enter Weather Condition (Sunny, Rainy, Cloudy):  Sunny\n",
      "Enter Days to Harvest:  85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for new data: 445.15591147521786\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the trained model coefficients and intercept\n",
    "global_coef, global_intercept = joblib.load('global_model.pkl')\n",
    "\n",
    "# Correct mappings\n",
    "soil_type_mapping = {\n",
    "    'Sandy': 4,\n",
    "    'Clay': 1,\n",
    "    'Loam': 2,\n",
    "    'Silt': 5,\n",
    "    'Peaty': 3,\n",
    "    'Chalky': 0\n",
    "}\n",
    "\n",
    "crop_mapping = {\n",
    "    'Cotton': 1,\n",
    "    'Rice': 3,\n",
    "    'Barley': 0,\n",
    "    'Soybean': 4,\n",
    "    'Wheat': 5,\n",
    "    'Maize': 2\n",
    "}\n",
    "\n",
    "weather_condition_mapping = {\n",
    "    'Sunny': 2,\n",
    "    'Rainy': 1,\n",
    "    'Cloudy': 0\n",
    "}\n",
    "\n",
    "# Function to process new input data and make predictions\n",
    "def predict_crop_yield(input_data):\n",
    "    # Convert input data to DataFrame\n",
    "    input_df = pd.DataFrame([input_data])\n",
    "    \n",
    "    # Apply the same preprocessing steps as before\n",
    "    input_df['Soil_Type'] = input_df['Soil_Type'].map(soil_type_mapping)\n",
    "    input_df['Crop'] = input_df['Crop'].map(crop_mapping)\n",
    "    input_df['Weather_Condition'] = input_df['Weather_Condition'].map(weather_condition_mapping)\n",
    "    input_df['Fertilizer_Used'] = input_df['Fertilizer_Used'].map({'Yes': 1, 'No': 0})\n",
    "    input_df['Irrigation_Used'] = input_df['Irrigation_Used'].map({'Yes': 1, 'No': 0})\n",
    "    \n",
    "    # Convert the DataFrame to a numpy array\n",
    "    input_data_array = input_df.to_numpy().flatten()\n",
    "\n",
    "    # Ensure the number of features matches the model's coefficients\n",
    "    if len(input_data_array) != len(global_coef):\n",
    "        raise ValueError(f\"Feature count mismatch: Expected {len(global_coef)} features, but got {len(input_data_array)}.\")\n",
    "\n",
    "    # Get the predicted crop yield using the model's coefficients and intercept\n",
    "    prediction = np.dot(input_data_array, global_coef) + global_intercept\n",
    "\n",
    "    return prediction\n",
    "\n",
    "# Function to get user input for each feature\n",
    "def get_user_input():\n",
    "    soil_type = input(\"Enter Soil Type (Sandy, Clay, Loam, Silt, Peaty, Chalky): \")\n",
    "    crop = input(\"Enter Crop (Cotton, Rice, Barley, Soybean, Wheat, Maize): \")\n",
    "    rainfall_mm = float(input(\"Enter Rainfall in mm: \"))\n",
    "    temperature_celsius = float(input(\"Enter Temperature in Celsius: \"))\n",
    "    fertilizer_used = input(\"Fertilizer Used (Yes, No): \")\n",
    "    irrigation_used = input(\"Irrigation Used (Yes, No): \")\n",
    "    weather_condition = input(\"Enter Weather Condition (Sunny, Rainy, Cloudy): \")\n",
    "    days_to_harvest = int(input(\"Enter Days to Harvest: \"))\n",
    "    \n",
    "    input_data = {\n",
    "        'Soil_Type': soil_type,\n",
    "        'Crop': crop,\n",
    "        'Rainfall_mm': rainfall_mm,\n",
    "        'Temperature_Celsius': temperature_celsius,\n",
    "        'Fertilizer_Used': fertilizer_used,\n",
    "        'Irrigation_Used': irrigation_used,\n",
    "        'Weather_Condition': weather_condition,\n",
    "        'Days_to_Harvest': days_to_harvest\n",
    "    }\n",
    "    \n",
    "    return input_data\n",
    "\n",
    "# Get user input\n",
    "input_data = get_user_input()\n",
    "\n",
    "# Get the predicted crop yield\n",
    "predicted_yield = predict_crop_yield(input_data)\n",
    "print(\"Prediction for new data:\", predicted_yield)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fa8871-3aba-4b90-aed8-2c5c71106c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "! streamlit run app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fa4f170b-a16f-409f-b7c5-73af3f7f84af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Soil_Type      Crop  Rainfall_mm  Temperature_Celsius  \\\n",
      "0        0.877795 -0.877956     1.335747             0.023821   \n",
      "1       -0.878529  0.293144     1.703634            -1.312747   \n",
      "2       -0.293088 -1.463506    -1.546977             0.317020   \n",
      "3        0.877795  0.878694     1.681287            -1.504137   \n",
      "4        1.463237  1.464244     0.694233             0.569997   \n",
      "...           ...       ...          ...                  ...   \n",
      "999995   1.463237  0.293144    -0.951223             0.066817   \n",
      "999996  -1.463971 -1.463506     1.473957             1.683526   \n",
      "999997   0.292354 -0.877956     1.221392            -0.434164   \n",
      "999998   1.463237  1.464244    -0.220007             0.767324   \n",
      "999999   0.877795 -0.292406    -1.420219            -0.028546   \n",
      "\n",
      "        Fertilizer_Used  Irrigation_Used  Weather_Condition  Days_to_Harvest  \\\n",
      "0              -0.99988         1.001019          -1.226353         0.674477   \n",
      "1               1.00012         1.001019          -0.001398         1.368028   \n",
      "2              -0.99988        -0.998983           1.223558         0.057988   \n",
      "3              -0.99988         1.001019          -0.001398         1.599212   \n",
      "4               1.00012         1.001019          -1.226353         0.212110   \n",
      "...                 ...              ...                ...              ...   \n",
      "999995         -0.99988        -0.998983           1.223558        -1.097930   \n",
      "999996          1.00012        -0.998983          -0.001398        -0.442910   \n",
      "999997          1.00012        -0.998983          -1.226353         0.135049   \n",
      "999998         -0.99988        -0.998983           1.223558        -0.096135   \n",
      "999999          1.00012        -0.998983           1.223558        -1.097930   \n",
      "\n",
      "        Yield_tons_per_hectare  \n",
      "0                     1.123645  \n",
      "1                     2.285709  \n",
      "2                    -2.075968  \n",
      "3                     1.101103  \n",
      "4                     1.531783  \n",
      "...                        ...  \n",
      "999995               -1.946211  \n",
      "999996                1.569118  \n",
      "999997                0.656447  \n",
      "999998               -1.520309  \n",
      "999999               -1.009229  \n",
      "\n",
      "[1000000 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SATYAJIT\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:347: InconsistentVersionWarning: Trying to unpickle estimator LinearRegression from version 1.3.2 when using version 1.3.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\SATYAJIT\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:457: UserWarning: X has feature names, but LinearRegression was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 8 features, but LinearRegression is expecting 17 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 87\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m     86\u001b[0m \u001b[38;5;66;03m# Run evaluation\u001b[39;00m\n\u001b[1;32m---> 87\u001b[0m results \u001b[38;5;241m=\u001b[39m evaluate_models(X_test, y_test, client_models, global_coef, global_intercept)\n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m# Print results in a formatted way\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mModel Evaluation Results:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[23], line 55\u001b[0m, in \u001b[0;36mevaluate_models\u001b[1;34m(X_test, y_test, client_models, global_coef, global_intercept)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Evaluate each client model\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m client_id, model \u001b[38;5;129;01min\u001b[39;00m client_models\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;66;03m# Get predictions\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m     56\u001b[0m     y_pred_proba \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict_proba(X_test)[:, \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;66;03m# Calculate metrics\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_base.py:386\u001b[0m, in \u001b[0;36mLinearModel.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    373\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;124;03m    Predict using the linear model.\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;124;03m        Returns predicted values.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 386\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decision_function(X)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_base.py:369\u001b[0m, in \u001b[0;36mLinearModel._decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_decision_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    367\u001b[0m     check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 369\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(X, accept_sparse\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoo\u001b[39m\u001b[38;5;124m\"\u001b[39m], reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    370\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m safe_sparse_dot(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_\u001b[38;5;241m.\u001b[39mT, dense_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:625\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    622\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    624\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 625\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_n_features(X, reset\u001b[38;5;241m=\u001b[39mreset)\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:414\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    411\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    413\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 414\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    415\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    416\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    417\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 8 features, but LinearRegression is expecting 17 features as input."
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, classification_report, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "\n",
    "df = pd.read_csv('crop_yield.csv')\n",
    "df = df.drop(columns=['Region'])\n",
    "df['Soil_Type'] = label_encoder.fit_transform(df['Soil_Type'])\n",
    "df['Crop'] = label_encoder.fit_transform(df['Crop'])\n",
    "df['Weather_Condition'] = label_encoder.fit_transform(df['Weather_Condition'])\n",
    "df['Irrigation_Used'] = label_encoder.fit_transform(df['Irrigation_Used'])\n",
    "df['Fertilizer_Used'] = label_encoder.fit_transform(df['Fertilizer_Used'])\n",
    "df1=df.copy()\n",
    "df2=df.copy()\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "df3 = scaler.fit_transform(df1)\n",
    "\n",
    "# Convert the result back to a DataFrame with column names from df2\n",
    "df3 = pd.DataFrame(df3, columns=df1.columns)\n",
    "\n",
    "# Display the standardized DataFrame\n",
    "print(df3)\n",
    "X = df3.drop(['Yield_tons_per_hectare'],axis=1)\n",
    "y = df3['Yield_tons_per_hectare']\n",
    "\n",
    "# Split into train/test if you haven't already\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Load the models\n",
    "client_models = {\n",
    "    1: joblib.load('model_client_1.pkl'),\n",
    "    2: joblib.load('model_client_2.pkl'),\n",
    "    3: joblib.load('model_client_3.pkl'),\n",
    "    4: joblib.load('model_client_4.pkl')\n",
    "}\n",
    "\n",
    "# Load the global model\n",
    "global_coef, global_intercept = joblib.load('global_model.pkl')\n",
    "\n",
    "def evaluate_models(X_test, y_test, client_models, global_coef, global_intercept):\n",
    "    results = {}\n",
    "    \n",
    "    # Evaluate each client model\n",
    "    for client_id, model in client_models.items():\n",
    "        # Get predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        report = classification_report(y_test, y_pred)\n",
    "        \n",
    "        results[f'Client_{client_id}'] = {\n",
    "            'accuracy': accuracy,\n",
    "            'auc': auc,\n",
    "            'classification_report': report\n",
    "        }\n",
    "    \n",
    "    # Evaluate global model\n",
    "    # For global model, we need to manually calculate predictions\n",
    "    global_pred = np.dot(X_test, global_coef) + global_intercept\n",
    "    global_pred_binary = (global_pred > 0.5).astype(int)\n",
    "    \n",
    "    global_accuracy = accuracy_score(y_test, global_pred_binary)\n",
    "    global_auc = roc_auc_score(y_test, global_pred)\n",
    "    global_report = classification_report(y_test, global_pred_binary)\n",
    "    \n",
    "    results['Global'] = {\n",
    "        'accuracy': global_accuracy,\n",
    "        'auc': global_auc,\n",
    "        'classification_report': global_report\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run evaluation\n",
    "results = evaluate_models(X_test, y_test, client_models, global_coef, global_intercept)\n",
    "\n",
    "# Print results in a formatted way\n",
    "print(\"\\nModel Evaluation Results:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"AUC-ROC: {metrics['auc']:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(metrics['classification_report'])\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Compare models\n",
    "accuracies = {model: metrics['accuracy'] for model, metrics in results.items()}\n",
    "best_model = max(accuracies, key=accuracies.get)\n",
    "print(f\"\\nBest performing model: {best_model} with accuracy: {accuracies[best_model]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "669719a6-a126-4772-accc-c2a5c385b2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features in test data: 8\n",
      "Features in test data: ['Soil_Type', 'Crop', 'Rainfall_mm', 'Temperature_Celsius', 'Fertilizer_Used', 'Irrigation_Used', 'Weather_Condition', 'Days_to_Harvest']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SATYAJIT\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:347: InconsistentVersionWarning: Trying to unpickle estimator LinearRegression from version 1.3.2 when using version 1.3.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Print feature information\n",
    "print(\"Number of features in test data:\", X_test.shape[1])\n",
    "print(\"Features in test data:\", list(X_test.columns))  # if using pandas\n",
    "\n",
    "# Load one of the client models to check expected features\n",
    "model = joblib.load('model_client_1.pkl')\n",
    "if hasattr(model, 'feature_names_in_'):\n",
    "    print(\"Features expected by model:\", list(model.feature_names_in_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "285a135b-89f1-4cca-8224-441e29cad260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model expects 17 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SATYAJIT\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:347: InconsistentVersionWarning: Trying to unpickle estimator LinearRegression from version 1.3.2 when using version 1.3.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load one client model to check expected features\n",
    "model = joblib.load('model_client_1.pkl')\n",
    "if hasattr(model, 'feature_names_in_'):\n",
    "    print(\"Features expected by model:\", list(model.feature_names_in_))\n",
    "else:\n",
    "    # If feature names aren't stored, we at least know it expects 17 features\n",
    "    print(\"Model expects 17 features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7e50a803-cb3c-4d21-9a11-6a859b34fe09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SATYAJIT\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\SATYAJIT\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:347: InconsistentVersionWarning: Trying to unpickle estimator LinearRegression from version 1.3.2 when using version 1.3.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\SATYAJIT\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:457: UserWarning: X has feature names, but LinearRegression was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features after preprocessing: ['Rainfall_mm', 'Temperature_Celsius', 'Fertilizer_Used', 'Irrigation_Used', 'Days_to_Harvest', 'Soil_Type_-0.8785294771289613', 'Soil_Type_-0.2930878736332484', 'Soil_Type_0.29235372986246455', 'Soil_Type_0.8777953333581775', 'Soil_Type_1.4632369368538904', 'Crop_-0.8779560852889114', 'Crop_-0.29240609743473833', 'Crop_0.2931438904194347', 'Crop_0.8786938782736078', 'Crop_1.4642438661277808', 'Weather_Condition_-0.0013976739363985857', 'Weather_Condition_1.2235575726880659']\n",
      "Number of features after preprocessing: 17\n",
      "\n",
      "Original test data shape: (200000, 8)\n",
      "Processed test data shape: (200000, 17)\n",
      "Error evaluating Client_3: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- Crop_-0.29240609743473833\n",
      "- Crop_-0.8779560852889114\n",
      "- Crop_0.2931438904194347\n",
      "- Crop_0.8786938782736078\n",
      "- Crop_1.4642438661277808\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- Crop\n",
      "- Soil_Type\n",
      "- Weather_Condition\n",
      "\n",
      "Error evaluating Client_4: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- Crop_-0.29240609743473833\n",
      "- Crop_-0.8779560852889114\n",
      "- Crop_0.2931438904194347\n",
      "- Crop_0.8786938782736078\n",
      "- Crop_1.4642438661277808\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- Crop\n",
      "- Soil_Type\n",
      "- Weather_Condition\n",
      "\n",
      "Error evaluating Global model: shapes (200000,17) and (8,) not aligned: 17 (dim 1) != 8 (dim 0)\n",
      "\n",
      "Model Evaluation Results:\n",
      "--------------------------------------------------\n",
      "\n",
      "Client_1:\n",
      "RMSE: 4.7031\n",
      "MSE: 22.1190\n",
      "\n",
      "Client_2:\n",
      "RMSE: 4.6999\n",
      "MSE: 22.0894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SATYAJIT\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:457: UserWarning: X has feature names, but LinearRegression was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def preprocess_data(df):\n",
    "    \"\"\"\n",
    "    Preprocess the data to match the model's expected format\n",
    "    \"\"\"\n",
    "    # Make a copy to avoid modifying original data\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. Handle categorical variables\n",
    "    categorical_features = ['Soil_Type', 'Crop', 'Weather_Condition']\n",
    "    numerical_features = ['Rainfall_mm', 'Temperature_Celsius', 'Fertilizer_Used', \n",
    "                         'Irrigation_Used', 'Days_to_Harvest']\n",
    "    \n",
    "    # Create column transformer for categorical variables\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', 'passthrough', numerical_features),\n",
    "            ('cat', OneHotEncoder(drop='first', sparse=False), categorical_features)\n",
    "        ])\n",
    "    \n",
    "    # Fit and transform the data\n",
    "    X_processed = preprocessor.fit_transform(df)\n",
    "    \n",
    "    # Get feature names after encoding\n",
    "    cat_feature_names = []\n",
    "    for i, feature in enumerate(categorical_features):\n",
    "        categories = preprocessor.named_transformers_['cat'].categories_[i][1:]  # drop first category\n",
    "        cat_feature_names.extend([f\"{feature}_{cat}\" for cat in categories])\n",
    "    \n",
    "    feature_names = numerical_features + cat_feature_names\n",
    "    \n",
    "    # Convert to DataFrame with proper column names\n",
    "    X_processed_df = pd.DataFrame(X_processed, columns=feature_names)\n",
    "    \n",
    "    print(\"Features after preprocessing:\", list(X_processed_df.columns))\n",
    "    print(\"Number of features after preprocessing:\", len(X_processed_df.columns))\n",
    "    \n",
    "    return X_processed_df, preprocessor\n",
    "\n",
    "# Preprocess your test data\n",
    "X_processed, preprocessor = preprocess_data(X_test)\n",
    "\n",
    "# Now check if the number of features matches\n",
    "print(\"\\nOriginal test data shape:\", X_test.shape)\n",
    "print(\"Processed test data shape:\", X_processed.shape)\n",
    "\n",
    "# If the number of features still doesn't match 17, we'll need to:\n",
    "# 1. Check what features the model was trained on\n",
    "# 2. Add any missing features with default values\n",
    "# 3. Ensure feature order matches the training data\n",
    "\n",
    "# Load and evaluate models with processed data\n",
    "def evaluate_models_with_processed_data(X_processed, y_test, client_models, global_coef, global_intercept):\n",
    "    results = {}\n",
    "    \n",
    "    # Evaluate each client model\n",
    "    for client_id, model in client_models.items():\n",
    "        try:\n",
    "            # Get predictions\n",
    "            y_pred = model.predict(X_processed)\n",
    "            \n",
    "            # Calculate metrics (assuming regression task)\n",
    "            mse = mean_squared_error(y_test, y_pred)\n",
    "            rmse = np.sqrt(mse)\n",
    "            \n",
    "            results[f'Client_{client_id}'] = {\n",
    "                'rmse': rmse,\n",
    "                'mse': mse\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating Client_{client_id}: {str(e)}\")\n",
    "    \n",
    "    # Evaluate global model\n",
    "    try:\n",
    "        global_pred = np.dot(X_processed, global_coef) + global_intercept\n",
    "        global_mse = mean_squared_error(y_test, global_pred)\n",
    "        global_rmse = np.sqrt(global_mse)\n",
    "        \n",
    "        results['Global'] = {\n",
    "            'rmse': global_rmse,\n",
    "            'mse': global_mse\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating Global model: {str(e)}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Load models and evaluate\n",
    "client_models = {\n",
    "    1: joblib.load('model_client_1.pkl'),\n",
    "    2: joblib.load('model_client_2.pkl'),\n",
    "    3: joblib.load('model_client_3.pkl'),\n",
    "    4: joblib.load('model_client_4.pkl')\n",
    "}\n",
    "\n",
    "global_coef, global_intercept = joblib.load('global_model.pkl')\n",
    "\n",
    "# Evaluate with processed data\n",
    "results = evaluate_models_with_processed_data(X_processed, y_test, client_models, global_coef, global_intercept)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nModel Evaluation Results:\")\n",
    "print(\"-\" * 50)\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"RMSE: {metrics['rmse']:.4f}\")\n",
    "    print(f\"MSE: {metrics['mse']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "22534649-ac15-401b-af09-be326a38cb4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features after preprocessing: ['Soil_Type', 'Crop', 'Rainfall_mm', 'Temperature_Celsius', 'Fertilizer_Used', 'Irrigation_Used', 'Weather_Condition', 'Days_to_Harvest']\n",
      "Number of features after preprocessing: 8\n",
      "Processed test data shape: (200000, 8)\n",
      "Error evaluating Client_1: X has 8 features, but LinearRegression is expecting 17 features as input.\n",
      "Error evaluating Client_2: X has 8 features, but LinearRegression is expecting 17 features as input.\n",
      "\n",
      "Model Evaluation Results:\n",
      "--------------------------------------------------\n",
      "\n",
      "Client_3:\n",
      "RMSE: 0.2952\n",
      "MSE: 0.0871\n",
      "R²: 0.9130\n",
      "\n",
      "Client_4:\n",
      "RMSE: 0.2952\n",
      "MSE: 0.0871\n",
      "R²: 0.9130\n",
      "\n",
      "Global:\n",
      "RMSE: 0.2952\n",
      "MSE: 0.0871\n",
      "R²: 0.9130\n",
      "\n",
      "Best performing model: Client_4\n",
      "Best RMSE: 0.2952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SATYAJIT\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:347: InconsistentVersionWarning: Trying to unpickle estimator LinearRegression from version 1.3.2 when using version 1.3.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\SATYAJIT\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:457: UserWarning: X has feature names, but LinearRegression was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\SATYAJIT\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:457: UserWarning: X has feature names, but LinearRegression was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def preprocess_data_simple(df):\n",
    "    \"\"\"\n",
    "    Preprocess the data keeping original feature names without one-hot encoding\n",
    "    \"\"\"\n",
    "    # Make a copy to avoid modifying original data\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Initialize label encoders for categorical variables\n",
    "    categorical_features = ['Soil_Type', 'Crop', 'Weather_Condition']\n",
    "    label_encoders = {}\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    for feature in categorical_features:\n",
    "        label_encoders[feature] = LabelEncoder()\n",
    "        df[feature] = label_encoders[feature].fit_transform(df[feature])\n",
    "    \n",
    "    # Ensure all features are numeric\n",
    "    for col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col])\n",
    "    \n",
    "    # Get the feature order to match the original training data\n",
    "    feature_order = [\n",
    "        'Soil_Type',\n",
    "        'Crop',\n",
    "        'Rainfall_mm',\n",
    "        'Temperature_Celsius',\n",
    "        'Fertilizer_Used',\n",
    "        'Irrigation_Used',\n",
    "        'Weather_Condition',\n",
    "        'Days_to_Harvest'\n",
    "    ]\n",
    "    \n",
    "    return df[feature_order]\n",
    "\n",
    "# Preprocess test data\n",
    "X_processed = preprocess_data_simple(X_test)\n",
    "\n",
    "print(\"Features after preprocessing:\", list(X_processed.columns))\n",
    "print(\"Number of features after preprocessing:\", len(X_processed.columns))\n",
    "print(\"Processed test data shape:\", X_processed.shape)\n",
    "\n",
    "# Load and evaluate models\n",
    "def evaluate_models(X_processed, y_test, client_models, global_coef, global_intercept):\n",
    "    results = {}\n",
    "    \n",
    "    # Evaluate each client model\n",
    "    for client_id, model in client_models.items():\n",
    "        try:\n",
    "            # Get predictions\n",
    "            y_pred = model.predict(X_processed)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            mse = mean_squared_error(y_test, y_pred)\n",
    "            rmse = np.sqrt(mse)\n",
    "            \n",
    "            # Calculate R-squared\n",
    "            ss_tot = np.sum((y_test - np.mean(y_test)) ** 2)\n",
    "            ss_res = np.sum((y_test - y_pred) ** 2)\n",
    "            r2 = 1 - (ss_res / ss_tot)\n",
    "            \n",
    "            results[f'Client_{client_id}'] = {\n",
    "                'rmse': rmse,\n",
    "                'mse': mse,\n",
    "                'r2': r2\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating Client_{client_id}: {str(e)}\")\n",
    "    \n",
    "    # Evaluate global model\n",
    "    try:\n",
    "        global_pred = np.dot(X_processed, global_coef) + global_intercept\n",
    "        global_mse = mean_squared_error(y_test, global_pred)\n",
    "        global_rmse = np.sqrt(global_mse)\n",
    "        \n",
    "        # Calculate R-squared for global model\n",
    "        ss_tot = np.sum((y_test - np.mean(y_test)) ** 2)\n",
    "        ss_res = np.sum((y_test - global_pred) ** 2)\n",
    "        global_r2 = 1 - (ss_res / ss_tot)\n",
    "        \n",
    "        results['Global'] = {\n",
    "            'rmse': global_rmse,\n",
    "            'mse': global_mse,\n",
    "            'r2': global_r2\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating Global model: {str(e)}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Load models\n",
    "client_models = {\n",
    "    1: joblib.load('model_client_1.pkl'),\n",
    "    2: joblib.load('model_client_2.pkl'),\n",
    "    3: joblib.load('model_client_3.pkl'),\n",
    "    4: joblib.load('model_client_4.pkl')\n",
    "}\n",
    "\n",
    "global_coef, global_intercept = joblib.load('global_model.pkl')\n",
    "\n",
    "# Evaluate models\n",
    "results = evaluate_models(X_processed, y_test, client_models, global_coef, global_intercept)\n",
    "\n",
    "# Print results with additional details\n",
    "print(\"\\nModel Evaluation Results:\")\n",
    "print(\"-\" * 50)\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"RMSE: {metrics['rmse']:.4f}\")\n",
    "    print(f\"MSE: {metrics['mse']:.4f}\")\n",
    "    print(f\"R²: {metrics['r2']:.4f}\")\n",
    "\n",
    "# Find best performing model\n",
    "best_model = min(results.items(), key=lambda x: x[1]['rmse'])\n",
    "print(f\"\\nBest performing model: {best_model[0]}\")\n",
    "print(f\"Best RMSE: {best_model[1]['rmse']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "359beaf3-5e22-4d5d-bcd3-97465a1410b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distributing global model to clients...\n",
      "--------------------------------------------------\n",
      "Client 1: ✓\n",
      "  Model: distributed_models\\client_1\\global_model_v20241222_163858.pkl\n",
      "  Metadata: distributed_models\\client_1\\metadata_v20241222_163858.json\n",
      "Client 2: ✓\n",
      "  Model: distributed_models\\client_2\\global_model_v20241222_163858.pkl\n",
      "  Metadata: distributed_models\\client_2\\metadata_v20241222_163858.json\n",
      "Client 3: ✓\n",
      "  Model: distributed_models\\client_3\\global_model_v20241222_163858.pkl\n",
      "  Metadata: distributed_models\\client_3\\metadata_v20241222_163858.json\n",
      "Client 4: ✓\n",
      "  Model: distributed_models\\client_4\\global_model_v20241222_163858.pkl\n",
      "  Metadata: distributed_models\\client_4\\metadata_v20241222_163858.json\n",
      "\n",
      "Verifying distributions...\n",
      "--------------------------------------------------\n",
      "Client 1: ✓\n",
      "Client 2: ✓\n",
      "Client 3: ✓\n",
      "Client 4: ✓\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "\n",
    "class ModelDistributor:\n",
    "    def __init__(self, global_model_path='global_model.pkl', version=None):\n",
    "        self.global_model_path = global_model_path\n",
    "        self.version = version or datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        self.distribution_dir = Path('distributed_models')\n",
    "        self.distribution_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "    def calculate_model_hash(self, model_data):\n",
    "        \"\"\"Calculate SHA-256 hash of model data for integrity verification\"\"\"\n",
    "        return hashlib.sha256(str(model_data).encode()).hexdigest()\n",
    "    \n",
    "    def create_model_metadata(self, client_id, model_hash):\n",
    "        \"\"\"Create metadata for the distributed model\"\"\"\n",
    "        return {\n",
    "            'client_id': client_id,\n",
    "            'distribution_date': datetime.now().isoformat(),\n",
    "            'model_version': self.version,\n",
    "            'model_hash': model_hash,\n",
    "            'original_model': self.global_model_path\n",
    "        }\n",
    "    \n",
    "    def distribute_to_client(self, client_id):\n",
    "        \"\"\"Distribute global model to a specific client\"\"\"\n",
    "        try:\n",
    "            # Load global model\n",
    "            global_coef, global_intercept = joblib.load(self.global_model_path)\n",
    "            \n",
    "            # Create client-specific model directory\n",
    "            client_dir = self.distribution_dir / f'client_{client_id}'\n",
    "            client_dir.mkdir(exist_ok=True)\n",
    "            \n",
    "            # Create model file name with version\n",
    "            model_filename = f'global_model_v{self.version}.pkl'\n",
    "            model_path = client_dir / model_filename\n",
    "            \n",
    "            # Save model for client\n",
    "            model_data = (global_coef, global_intercept)\n",
    "            joblib.dump(model_data, model_path)\n",
    "            \n",
    "            # Calculate model hash\n",
    "            model_hash = self.calculate_model_hash(model_data)\n",
    "            \n",
    "            # Create and save metadata\n",
    "            metadata = self.create_model_metadata(client_id, model_hash)\n",
    "            metadata_path = client_dir / f'metadata_v{self.version}.json'\n",
    "            \n",
    "            with open(metadata_path, 'w') as f:\n",
    "                json.dump(metadata, f, indent=4)\n",
    "            \n",
    "            return {\n",
    "                'status': 'success',\n",
    "                'client_id': client_id,\n",
    "                'model_path': str(model_path),\n",
    "                'metadata_path': str(metadata_path)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'status': 'error',\n",
    "                'client_id': client_id,\n",
    "                'error': str(e)\n",
    "            }\n",
    "    \n",
    "    def distribute_to_all_clients(self, client_ids):\n",
    "        \"\"\"Distribute global model to all clients\"\"\"\n",
    "        results = []\n",
    "        for client_id in client_ids:\n",
    "            result = self.distribute_to_client(client_id)\n",
    "            results.append(result)\n",
    "            \n",
    "            # Print status message\n",
    "            status = \"✓\" if result['status'] == 'success' else \"✗\"\n",
    "            print(f\"Client {client_id}: {status}\")\n",
    "            \n",
    "            if result['status'] == 'error':\n",
    "                print(f\"  Error: {result['error']}\")\n",
    "            else:\n",
    "                print(f\"  Model: {result['model_path']}\")\n",
    "                print(f\"  Metadata: {result['metadata_path']}\")\n",
    "                \n",
    "        return results\n",
    "    \n",
    "    def verify_distribution(self, client_id):\n",
    "        \"\"\"Verify the integrity of a distributed model\"\"\"\n",
    "        try:\n",
    "            client_dir = self.distribution_dir / f'client_{client_id}'\n",
    "            model_path = client_dir / f'global_model_v{self.version}.pkl'\n",
    "            metadata_path = client_dir / f'metadata_v{self.version}.json'\n",
    "            \n",
    "            # Load distributed model and calculate hash\n",
    "            distributed_model = joblib.load(model_path)\n",
    "            current_hash = self.calculate_model_hash(distributed_model)\n",
    "            \n",
    "            # Load metadata and get stored hash\n",
    "            with open(metadata_path, 'r') as f:\n",
    "                metadata = json.load(f)\n",
    "            stored_hash = metadata['model_hash']\n",
    "            \n",
    "            # Compare hashes\n",
    "            return {\n",
    "                'client_id': client_id,\n",
    "                'verified': current_hash == stored_hash,\n",
    "                'model_version': metadata['model_version'],\n",
    "                'distribution_date': metadata['distribution_date']\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'client_id': client_id,\n",
    "                'verified': False,\n",
    "                'error': str(e)\n",
    "            }\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize distributor\n",
    "    distributor = ModelDistributor(global_model_path='global_model.pkl')\n",
    "    \n",
    "    # List of client IDs\n",
    "    client_ids = [1, 2, 3, 4]\n",
    "    \n",
    "    print(\"Distributing global model to clients...\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Distribute models\n",
    "    results = distributor.distribute_to_all_clients(client_ids)\n",
    "    \n",
    "    print(\"\\nVerifying distributions...\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Verify distributions\n",
    "    for client_id in client_ids:\n",
    "        verification = distributor.verify_distribution(client_id)\n",
    "        status = \"✓\" if verification.get('verified') else \"✗\"\n",
    "        print(f\"Client {client_id}: {status}\")\n",
    "        if not verification.get('verified'):\n",
    "            print(f\"  Error: {verification.get('error', 'Verification failed')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47bc27a1-5aba-48c5-b74b-b1deab90f526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 1 Coefficients: [ 1.29646234e+00  1.43479168e-01  7.49492860e-01  5.99692963e-01\n",
      "  1.20276851e-03  2.19722231e-03 -7.63652704e-04 -1.11523257e-03\n",
      "  1.09437005e-03  1.98662205e-03 -2.13774932e-03 -4.83673753e-03\n",
      "  2.75819816e-03 -6.03955228e-03 -4.41903616e-03 -3.82805169e-04\n",
      "  1.72747181e-03]\n",
      "Client 1 Intercept: 4.649473449370341\n",
      "Client 2 Coefficients: [ 1.29855473e+00  1.43170765e-01  7.51797574e-01  6.00038871e-01\n",
      "  9.00648904e-04  2.16979625e-03 -4.38318816e-03 -7.64832536e-05\n",
      "  4.68796958e-04 -6.50306727e-03  4.78925832e-03  3.99148694e-03\n",
      "  2.06905597e-03  3.06729233e-03  5.09248100e-03 -2.31231230e-04\n",
      "  5.65013950e-04]\n",
      "Client 2 Intercept: 4.642552360196861\n",
      "Client 3 Coefficients: [ 1.54702547e-04 -1.22302337e-03  7.64345295e-01  8.48115199e-02\n",
      "  4.41670814e-01  3.53623120e-01  5.37683894e-04  5.90079238e-05]\n",
      "Client 3 Intercept: 5.774058109752646e-05\n",
      "Client 4 Coefficients: [ 3.53241478e-04 -8.53621509e-04  7.66118385e-01  8.47888754e-02\n",
      "  4.41010276e-01  3.52858297e-01 -4.85743316e-04  1.15542825e-03]\n",
      "Client 4 Intercept: 0.0005938679868076994\n",
      "Model parameters saved successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SATYAJIT\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:347: InconsistentVersionWarning: Trying to unpickle estimator LinearRegression from version 1.3.2 when using version 1.3.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "# Load the models\n",
    "model_client_1 = joblib.load('model_client_1.pkl')\n",
    "model_client_2 = joblib.load('model_client_2.pkl')\n",
    "model_client_3 = joblib.load('model_client_3.pkl')\n",
    "model_client_4 = joblib.load('model_client_4.pkl')\n",
    "\n",
    "# Extract weights (coefficients)\n",
    "coef_1 = model_client_1.coef_\n",
    "coef_2 = model_client_2.coef_\n",
    "coef_3 = model_client_3.coef_\n",
    "coef_4 = model_client_4.coef_\n",
    "\n",
    "# Extract biases (intercepts)\n",
    "intercept_1 = model_client_1.intercept_\n",
    "intercept_2 = model_client_2.intercept_\n",
    "intercept_3 = model_client_3.intercept_\n",
    "intercept_4 = model_client_4.intercept_\n",
    "\n",
    "# Display results\n",
    "print(\"Client 1 Coefficients:\", coef_1)\n",
    "print(\"Client 1 Intercept:\", intercept_1)\n",
    "\n",
    "print(\"Client 2 Coefficients:\", coef_2)\n",
    "print(\"Client 2 Intercept:\", intercept_2)\n",
    "\n",
    "print(\"Client 3 Coefficients:\", coef_3)\n",
    "print(\"Client 3 Intercept:\", intercept_3)\n",
    "\n",
    "print(\"Client 4 Coefficients:\", coef_4)\n",
    "print(\"Client 4 Intercept:\", intercept_4)\n",
    "\n",
    "# Save coefficients and intercepts as .npy files\n",
    "np.save('coef_client_1.npy', coef_1)\n",
    "np.save('coef_client_2.npy', coef_2)\n",
    "np.save('coef_client_3.npy', coef_3)\n",
    "np.save('coef_client_4.npy', coef_4)\n",
    "\n",
    "np.save('intercept_client_1.npy', intercept_1)\n",
    "np.save('intercept_client_2.npy', intercept_2)\n",
    "np.save('intercept_client_3.npy', intercept_3)\n",
    "np.save('intercept_client_4.npy', intercept_4)\n",
    "\n",
    "print(\"Model parameters saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1198f30c-c1d3-4270-abca-7a27479a3060",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'git'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgit\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Initialize the repo and add remote\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'git'"
     ]
    }
   ],
   "source": [
    "import git\n",
    "import os\n",
    "\n",
    "# Initialize the repo and add remote\n",
    "repo_dir = r\"C:\\Users\\SATYAJIT\\crop yield\"  # Replace with the actual path\n",
    "repo = git.Repo.init(repo_dir)\n",
    "\n",
    "# Add remote (GitHub repository URL)\n",
    "origin = repo.create_remote('origin', 'https://github.com/555-Satyajit/Farm-params')  # Replace with your repo URL\n",
    "\n",
    "# Add files\n",
    "files_to_add = ['coef_client_1.npy', 'coef_client_2.npy', 'coef_client_3.npy', 'coef_client_4.npy',\n",
    "                'intercept_client_1.npy', 'intercept_client_2.npy', 'intercept_client_3.npy', 'intercept_client_4.npy']\n",
    "repo.index.add(files_to_add)\n",
    "\n",
    "# Commit the changes\n",
    "repo.index.commit(\"Upload model parameters\")\n",
    "\n",
    "# Push to GitHub\n",
    "origin.push('master')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "933cd976-3bf6-4c80-9fbc-35d6ecb39222",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement git (from versions: none)\n",
      "ERROR: No matching distribution found for git\n"
     ]
    }
   ],
   "source": [
    "! pip install git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f54f84d-ac8d-480f-9b42-366ae2998134",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
