{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26e68a91-1c1a-45d7-883e-177047fce802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 1 Coefficients: [ 1.54702547e-04 -1.22302337e-03  7.64345295e-01  8.48115199e-02\n",
      "  4.41670814e-01  3.53623120e-01  5.37683894e-04  5.90079238e-05]\n",
      "Client 1 Intercept: 5.774058109752646e-05\n",
      "Client 2 Coefficients: [ 1.54702547e-04 -1.22302337e-03  7.64345295e-01  8.48115199e-02\n",
      "  4.41670814e-01  3.53623120e-01  5.37683894e-04  5.90079238e-05]\n",
      "Client 2 Intercept: 5.774058109752646e-05\n",
      "Client 3 Coefficients: [ 1.54702547e-04 -1.22302337e-03  7.64345295e-01  8.48115199e-02\n",
      "  4.41670814e-01  3.53623120e-01  5.37683894e-04  5.90079238e-05]\n",
      "Client 3 Intercept: 5.774058109752646e-05\n",
      "Client 4 Coefficients: [ 3.53241478e-04 -8.53621509e-04  7.66118385e-01  8.47888754e-02\n",
      "  4.41010276e-01  3.52858297e-01 -4.85743316e-04  1.15542825e-03]\n",
      "Client 4 Intercept: 0.0005938679868076994\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Load the models\n",
    "model_client_1 = joblib.load('model_client_1.pkl')\n",
    "model_client_2 = joblib.load('model_client_2.pkl')\n",
    "model_client_3 = joblib.load('model_client_3.pkl')\n",
    "model_client_4 = joblib.load('model_client_4.pkl')\n",
    "\n",
    "# Extract weights (coefficients)\n",
    "coef_1 = model_client_1.coef_\n",
    "coef_2 = model_client_2.coef_\n",
    "coef_3 = model_client_3.coef_\n",
    "coef_4 = model_client_4.coef_\n",
    "\n",
    "# Extract biases (intercepts)\n",
    "intercept_1 = model_client_1.intercept_\n",
    "intercept_2 = model_client_2.intercept_\n",
    "intercept_3 = model_client_3.intercept_\n",
    "intercept_4 = model_client_4.intercept_\n",
    "\n",
    "# Display results\n",
    "print(\"Client 1 Coefficients:\", coef_1)\n",
    "print(\"Client 1 Intercept:\", intercept_1)\n",
    "\n",
    "print(\"Client 2 Coefficients:\", coef_2)\n",
    "print(\"Client 2 Intercept:\", intercept_2)\n",
    "\n",
    "print(\"Client 3 Coefficients:\", coef_3)\n",
    "print(\"Client 3 Intercept:\", intercept_3)\n",
    "\n",
    "print(\"Client 4 Coefficients:\", coef_4)\n",
    "print(\"Client 4 Intercept:\", intercept_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3d9013b-cbbe-4af7-83b5-49efb3953ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Coefficients: [ 2.04337280e-04 -1.13067290e-03  7.64788567e-01  8.48058588e-02\n",
      "  4.41505680e-01  3.53431914e-01  2.81827092e-04  3.33113006e-04]\n",
      "Global Intercept: 0.0001917724325250697\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Aggregate coefficients (average)\n",
    "global_coef = np.mean([coef_1, coef_2, coef_3, coef_4], axis=0)\n",
    "\n",
    "# Aggregate intercepts (average)\n",
    "global_intercept = np.mean([intercept_1, intercept_2, intercept_3, intercept_4])\n",
    "\n",
    "print(\"Global Coefficients:\", global_coef)\n",
    "print(\"Global Intercept:\", global_intercept)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b160c4c-1a1e-4383-9662-52497c3bd6cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['global_model.pkl']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump((global_coef, global_intercept), 'global_model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2efc9527-4a13-4c44-8678-b47214c002ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter Soil Type (Sandy, Clay, Loam, Silt, Peaty, Chalky):  Sandy\n",
      "Enter Crop (Cotton, Rice, Barley, Soybean, Wheat, Maize):  Rice\n",
      "Enter Rainfall in mm:  576\n",
      "Enter Temperature in Celsius:  45\n",
      "Fertilizer Used (Yes, No):  Yes\n",
      "Irrigation Used (Yes, No):  Yes\n",
      "Enter Weather Condition (Sunny, Rainy, Cloudy):  Sunny\n",
      "Enter Days to Harvest:  85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for new data: 445.15591147521786\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the trained model coefficients and intercept\n",
    "global_coef, global_intercept = joblib.load('global_model.pkl')\n",
    "\n",
    "# Correct mappings\n",
    "soil_type_mapping = {\n",
    "    'Sandy': 4,\n",
    "    'Clay': 1,\n",
    "    'Loam': 2,\n",
    "    'Silt': 5,\n",
    "    'Peaty': 3,\n",
    "    'Chalky': 0\n",
    "}\n",
    "\n",
    "crop_mapping = {\n",
    "    'Cotton': 1,\n",
    "    'Rice': 3,\n",
    "    'Barley': 0,\n",
    "    'Soybean': 4,\n",
    "    'Wheat': 5,\n",
    "    'Maize': 2\n",
    "}\n",
    "\n",
    "weather_condition_mapping = {\n",
    "    'Sunny': 2,\n",
    "    'Rainy': 1,\n",
    "    'Cloudy': 0\n",
    "}\n",
    "\n",
    "# Function to process new input data and make predictions\n",
    "def predict_crop_yield(input_data):\n",
    "    # Convert input data to DataFrame\n",
    "    input_df = pd.DataFrame([input_data])\n",
    "    \n",
    "    # Apply the same preprocessing steps as before\n",
    "    input_df['Soil_Type'] = input_df['Soil_Type'].map(soil_type_mapping)\n",
    "    input_df['Crop'] = input_df['Crop'].map(crop_mapping)\n",
    "    input_df['Weather_Condition'] = input_df['Weather_Condition'].map(weather_condition_mapping)\n",
    "    input_df['Fertilizer_Used'] = input_df['Fertilizer_Used'].map({'Yes': 1, 'No': 0})\n",
    "    input_df['Irrigation_Used'] = input_df['Irrigation_Used'].map({'Yes': 1, 'No': 0})\n",
    "    \n",
    "    # Convert the DataFrame to a numpy array\n",
    "    input_data_array = input_df.to_numpy().flatten()\n",
    "\n",
    "    # Ensure the number of features matches the model's coefficients\n",
    "    if len(input_data_array) != len(global_coef):\n",
    "        raise ValueError(f\"Feature count mismatch: Expected {len(global_coef)} features, but got {len(input_data_array)}.\")\n",
    "\n",
    "    # Get the predicted crop yield using the model's coefficients and intercept\n",
    "    prediction = np.dot(input_data_array, global_coef) + global_intercept\n",
    "\n",
    "    return prediction\n",
    "\n",
    "# Function to get user input for each feature\n",
    "def get_user_input():\n",
    "    soil_type = input(\"Enter Soil Type (Sandy, Clay, Loam, Silt, Peaty, Chalky): \")\n",
    "    crop = input(\"Enter Crop (Cotton, Rice, Barley, Soybean, Wheat, Maize): \")\n",
    "    rainfall_mm = float(input(\"Enter Rainfall in mm: \"))\n",
    "    temperature_celsius = float(input(\"Enter Temperature in Celsius: \"))\n",
    "    fertilizer_used = input(\"Fertilizer Used (Yes, No): \")\n",
    "    irrigation_used = input(\"Irrigation Used (Yes, No): \")\n",
    "    weather_condition = input(\"Enter Weather Condition (Sunny, Rainy, Cloudy): \")\n",
    "    days_to_harvest = int(input(\"Enter Days to Harvest: \"))\n",
    "    \n",
    "    input_data = {\n",
    "        'Soil_Type': soil_type,\n",
    "        'Crop': crop,\n",
    "        'Rainfall_mm': rainfall_mm,\n",
    "        'Temperature_Celsius': temperature_celsius,\n",
    "        'Fertilizer_Used': fertilizer_used,\n",
    "        'Irrigation_Used': irrigation_used,\n",
    "        'Weather_Condition': weather_condition,\n",
    "        'Days_to_Harvest': days_to_harvest\n",
    "    }\n",
    "    \n",
    "    return input_data\n",
    "\n",
    "# Get user input\n",
    "input_data = get_user_input()\n",
    "\n",
    "# Get the predicted crop yield\n",
    "predicted_yield = predict_crop_yield(input_data)\n",
    "print(\"Prediction for new data:\", predicted_yield)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fa8871-3aba-4b90-aed8-2c5c71106c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "! streamlit run app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fa4f170b-a16f-409f-b7c5-73af3f7f84af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Soil_Type      Crop  Rainfall_mm  Temperature_Celsius  \\\n",
      "0        0.877795 -0.877956     1.335747             0.023821   \n",
      "1       -0.878529  0.293144     1.703634            -1.312747   \n",
      "2       -0.293088 -1.463506    -1.546977             0.317020   \n",
      "3        0.877795  0.878694     1.681287            -1.504137   \n",
      "4        1.463237  1.464244     0.694233             0.569997   \n",
      "...           ...       ...          ...                  ...   \n",
      "999995   1.463237  0.293144    -0.951223             0.066817   \n",
      "999996  -1.463971 -1.463506     1.473957             1.683526   \n",
      "999997   0.292354 -0.877956     1.221392            -0.434164   \n",
      "999998   1.463237  1.464244    -0.220007             0.767324   \n",
      "999999   0.877795 -0.292406    -1.420219            -0.028546   \n",
      "\n",
      "        Fertilizer_Used  Irrigation_Used  Weather_Condition  Days_to_Harvest  \\\n",
      "0              -0.99988         1.001019          -1.226353         0.674477   \n",
      "1               1.00012         1.001019          -0.001398         1.368028   \n",
      "2              -0.99988        -0.998983           1.223558         0.057988   \n",
      "3              -0.99988         1.001019          -0.001398         1.599212   \n",
      "4               1.00012         1.001019          -1.226353         0.212110   \n",
      "...                 ...              ...                ...              ...   \n",
      "999995         -0.99988        -0.998983           1.223558        -1.097930   \n",
      "999996          1.00012        -0.998983          -0.001398        -0.442910   \n",
      "999997          1.00012        -0.998983          -1.226353         0.135049   \n",
      "999998         -0.99988        -0.998983           1.223558        -0.096135   \n",
      "999999          1.00012        -0.998983           1.223558        -1.097930   \n",
      "\n",
      "        Yield_tons_per_hectare  \n",
      "0                     1.123645  \n",
      "1                     2.285709  \n",
      "2                    -2.075968  \n",
      "3                     1.101103  \n",
      "4                     1.531783  \n",
      "...                        ...  \n",
      "999995               -1.946211  \n",
      "999996                1.569118  \n",
      "999997                0.656447  \n",
      "999998               -1.520309  \n",
      "999999               -1.009229  \n",
      "\n",
      "[1000000 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SATYAJIT\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:347: InconsistentVersionWarning: Trying to unpickle estimator LinearRegression from version 1.3.2 when using version 1.3.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\SATYAJIT\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:457: UserWarning: X has feature names, but LinearRegression was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 8 features, but LinearRegression is expecting 17 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 87\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m     86\u001b[0m \u001b[38;5;66;03m# Run evaluation\u001b[39;00m\n\u001b[1;32m---> 87\u001b[0m results \u001b[38;5;241m=\u001b[39m evaluate_models(X_test, y_test, client_models, global_coef, global_intercept)\n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m# Print results in a formatted way\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mModel Evaluation Results:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[23], line 55\u001b[0m, in \u001b[0;36mevaluate_models\u001b[1;34m(X_test, y_test, client_models, global_coef, global_intercept)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Evaluate each client model\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m client_id, model \u001b[38;5;129;01min\u001b[39;00m client_models\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;66;03m# Get predictions\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m     56\u001b[0m     y_pred_proba \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict_proba(X_test)[:, \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;66;03m# Calculate metrics\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_base.py:386\u001b[0m, in \u001b[0;36mLinearModel.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    373\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;124;03m    Predict using the linear model.\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;124;03m        Returns predicted values.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 386\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decision_function(X)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_base.py:369\u001b[0m, in \u001b[0;36mLinearModel._decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_decision_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    367\u001b[0m     check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 369\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(X, accept_sparse\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoo\u001b[39m\u001b[38;5;124m\"\u001b[39m], reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    370\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m safe_sparse_dot(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_\u001b[38;5;241m.\u001b[39mT, dense_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:625\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    622\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    624\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 625\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_n_features(X, reset\u001b[38;5;241m=\u001b[39mreset)\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:414\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    411\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    413\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 414\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    415\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    416\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    417\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 8 features, but LinearRegression is expecting 17 features as input."
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, classification_report, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "\n",
    "df = pd.read_csv('crop_yield.csv')\n",
    "df = df.drop(columns=['Region'])\n",
    "df['Soil_Type'] = label_encoder.fit_transform(df['Soil_Type'])\n",
    "df['Crop'] = label_encoder.fit_transform(df['Crop'])\n",
    "df['Weather_Condition'] = label_encoder.fit_transform(df['Weather_Condition'])\n",
    "df['Irrigation_Used'] = label_encoder.fit_transform(df['Irrigation_Used'])\n",
    "df['Fertilizer_Used'] = label_encoder.fit_transform(df['Fertilizer_Used'])\n",
    "df1=df.copy()\n",
    "df2=df.copy()\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "df3 = scaler.fit_transform(df1)\n",
    "\n",
    "# Convert the result back to a DataFrame with column names from df2\n",
    "df3 = pd.DataFrame(df3, columns=df1.columns)\n",
    "\n",
    "# Display the standardized DataFrame\n",
    "print(df3)\n",
    "X = df3.drop(['Yield_tons_per_hectare'],axis=1)\n",
    "y = df3['Yield_tons_per_hectare']\n",
    "\n",
    "# Split into train/test if you haven't already\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Load the models\n",
    "client_models = {\n",
    "    1: joblib.load('model_client_1.pkl'),\n",
    "    2: joblib.load('model_client_2.pkl'),\n",
    "    3: joblib.load('model_client_3.pkl'),\n",
    "    4: joblib.load('model_client_4.pkl')\n",
    "}\n",
    "\n",
    "# Load the global model\n",
    "global_coef, global_intercept = joblib.load('global_model.pkl')\n",
    "\n",
    "def evaluate_models(X_test, y_test, client_models, global_coef, global_intercept):\n",
    "    results = {}\n",
    "    \n",
    "    # Evaluate each client model\n",
    "    for client_id, model in client_models.items():\n",
    "        # Get predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        report = classification_report(y_test, y_pred)\n",
    "        \n",
    "        results[f'Client_{client_id}'] = {\n",
    "            'accuracy': accuracy,\n",
    "            'auc': auc,\n",
    "            'classification_report': report\n",
    "        }\n",
    "    \n",
    "    # Evaluate global model\n",
    "    # For global model, we need to manually calculate predictions\n",
    "    global_pred = np.dot(X_test, global_coef) + global_intercept\n",
    "    global_pred_binary = (global_pred > 0.5).astype(int)\n",
    "    \n",
    "    global_accuracy = accuracy_score(y_test, global_pred_binary)\n",
    "    global_auc = roc_auc_score(y_test, global_pred)\n",
    "    global_report = classification_report(y_test, global_pred_binary)\n",
    "    \n",
    "    results['Global'] = {\n",
    "        'accuracy': global_accuracy,\n",
    "        'auc': global_auc,\n",
    "        'classification_report': global_report\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run evaluation\n",
    "results = evaluate_models(X_test, y_test, client_models, global_coef, global_intercept)\n",
    "\n",
    "# Print results in a formatted way\n",
    "print(\"\\nModel Evaluation Results:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"AUC-ROC: {metrics['auc']:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(metrics['classification_report'])\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Compare models\n",
    "accuracies = {model: metrics['accuracy'] for model, metrics in results.items()}\n",
    "best_model = max(accuracies, key=accuracies.get)\n",
    "print(f\"\\nBest performing model: {best_model} with accuracy: {accuracies[best_model]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "669719a6-a126-4772-accc-c2a5c385b2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features in test data: 8\n",
      "Features in test data: ['Soil_Type', 'Crop', 'Rainfall_mm', 'Temperature_Celsius', 'Fertilizer_Used', 'Irrigation_Used', 'Weather_Condition', 'Days_to_Harvest']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SATYAJIT\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:347: InconsistentVersionWarning: Trying to unpickle estimator LinearRegression from version 1.3.2 when using version 1.3.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Print feature information\n",
    "print(\"Number of features in test data:\", X_test.shape[1])\n",
    "print(\"Features in test data:\", list(X_test.columns))  # if using pandas\n",
    "\n",
    "# Load one of the client models to check expected features\n",
    "model = joblib.load('model_client_1.pkl')\n",
    "if hasattr(model, 'feature_names_in_'):\n",
    "    print(\"Features expected by model:\", list(model.feature_names_in_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "285a135b-89f1-4cca-8224-441e29cad260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model expects 17 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SATYAJIT\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:347: InconsistentVersionWarning: Trying to unpickle estimator LinearRegression from version 1.3.2 when using version 1.3.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load one client model to check expected features\n",
    "model = joblib.load('model_client_1.pkl')\n",
    "if hasattr(model, 'feature_names_in_'):\n",
    "    print(\"Features expected by model:\", list(model.feature_names_in_))\n",
    "else:\n",
    "    # If feature names aren't stored, we at least know it expects 17 features\n",
    "    print(\"Model expects 17 features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7e50a803-cb3c-4d21-9a11-6a859b34fe09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SATYAJIT\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\SATYAJIT\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:347: InconsistentVersionWarning: Trying to unpickle estimator LinearRegression from version 1.3.2 when using version 1.3.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\SATYAJIT\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:457: UserWarning: X has feature names, but LinearRegression was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features after preprocessing: ['Rainfall_mm', 'Temperature_Celsius', 'Fertilizer_Used', 'Irrigation_Used', 'Days_to_Harvest', 'Soil_Type_-0.8785294771289613', 'Soil_Type_-0.2930878736332484', 'Soil_Type_0.29235372986246455', 'Soil_Type_0.8777953333581775', 'Soil_Type_1.4632369368538904', 'Crop_-0.8779560852889114', 'Crop_-0.29240609743473833', 'Crop_0.2931438904194347', 'Crop_0.8786938782736078', 'Crop_1.4642438661277808', 'Weather_Condition_-0.0013976739363985857', 'Weather_Condition_1.2235575726880659']\n",
      "Number of features after preprocessing: 17\n",
      "\n",
      "Original test data shape: (200000, 8)\n",
      "Processed test data shape: (200000, 17)\n",
      "Error evaluating Client_3: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- Crop_-0.29240609743473833\n",
      "- Crop_-0.8779560852889114\n",
      "- Crop_0.2931438904194347\n",
      "- Crop_0.8786938782736078\n",
      "- Crop_1.4642438661277808\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- Crop\n",
      "- Soil_Type\n",
      "- Weather_Condition\n",
      "\n",
      "Error evaluating Client_4: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- Crop_-0.29240609743473833\n",
      "- Crop_-0.8779560852889114\n",
      "- Crop_0.2931438904194347\n",
      "- Crop_0.8786938782736078\n",
      "- Crop_1.4642438661277808\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- Crop\n",
      "- Soil_Type\n",
      "- Weather_Condition\n",
      "\n",
      "Error evaluating Global model: shapes (200000,17) and (8,) not aligned: 17 (dim 1) != 8 (dim 0)\n",
      "\n",
      "Model Evaluation Results:\n",
      "--------------------------------------------------\n",
      "\n",
      "Client_1:\n",
      "RMSE: 4.7031\n",
      "MSE: 22.1190\n",
      "\n",
      "Client_2:\n",
      "RMSE: 4.6999\n",
      "MSE: 22.0894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SATYAJIT\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:457: UserWarning: X has feature names, but LinearRegression was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def preprocess_data(df):\n",
    "    \"\"\"\n",
    "    Preprocess the data to match the model's expected format\n",
    "    \"\"\"\n",
    "    # Make a copy to avoid modifying original data\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. Handle categorical variables\n",
    "    categorical_features = ['Soil_Type', 'Crop', 'Weather_Condition']\n",
    "    numerical_features = ['Rainfall_mm', 'Temperature_Celsius', 'Fertilizer_Used', \n",
    "                         'Irrigation_Used', 'Days_to_Harvest']\n",
    "    \n",
    "    # Create column transformer for categorical variables\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', 'passthrough', numerical_features),\n",
    "            ('cat', OneHotEncoder(drop='first', sparse=False), categorical_features)\n",
    "        ])\n",
    "    \n",
    "    # Fit and transform the data\n",
    "    X_processed = preprocessor.fit_transform(df)\n",
    "    \n",
    "    # Get feature names after encoding\n",
    "    cat_feature_names = []\n",
    "    for i, feature in enumerate(categorical_features):\n",
    "        categories = preprocessor.named_transformers_['cat'].categories_[i][1:]  # drop first category\n",
    "        cat_feature_names.extend([f\"{feature}_{cat}\" for cat in categories])\n",
    "    \n",
    "    feature_names = numerical_features + cat_feature_names\n",
    "    \n",
    "    # Convert to DataFrame with proper column names\n",
    "    X_processed_df = pd.DataFrame(X_processed, columns=feature_names)\n",
    "    \n",
    "    print(\"Features after preprocessing:\", list(X_processed_df.columns))\n",
    "    print(\"Number of features after preprocessing:\", len(X_processed_df.columns))\n",
    "    \n",
    "    return X_processed_df, preprocessor\n",
    "\n",
    "# Preprocess your test data\n",
    "X_processed, preprocessor = preprocess_data(X_test)\n",
    "\n",
    "# Now check if the number of features matches\n",
    "print(\"\\nOriginal test data shape:\", X_test.shape)\n",
    "print(\"Processed test data shape:\", X_processed.shape)\n",
    "\n",
    "# If the number of features still doesn't match 17, we'll need to:\n",
    "# 1. Check what features the model was trained on\n",
    "# 2. Add any missing features with default values\n",
    "# 3. Ensure feature order matches the training data\n",
    "\n",
    "# Load and evaluate models with processed data\n",
    "def evaluate_models_with_processed_data(X_processed, y_test, client_models, global_coef, global_intercept):\n",
    "    results = {}\n",
    "    \n",
    "    # Evaluate each client model\n",
    "    for client_id, model in client_models.items():\n",
    "        try:\n",
    "            # Get predictions\n",
    "            y_pred = model.predict(X_processed)\n",
    "            \n",
    "            # Calculate metrics (assuming regression task)\n",
    "            mse = mean_squared_error(y_test, y_pred)\n",
    "            rmse = np.sqrt(mse)\n",
    "            \n",
    "            results[f'Client_{client_id}'] = {\n",
    "                'rmse': rmse,\n",
    "                'mse': mse\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating Client_{client_id}: {str(e)}\")\n",
    "    \n",
    "    # Evaluate global model\n",
    "    try:\n",
    "        global_pred = np.dot(X_processed, global_coef) + global_intercept\n",
    "        global_mse = mean_squared_error(y_test, global_pred)\n",
    "        global_rmse = np.sqrt(global_mse)\n",
    "        \n",
    "        results['Global'] = {\n",
    "            'rmse': global_rmse,\n",
    "            'mse': global_mse\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating Global model: {str(e)}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Load models and evaluate\n",
    "client_models = {\n",
    "    1: joblib.load('model_client_1.pkl'),\n",
    "    2: joblib.load('model_client_2.pkl'),\n",
    "    3: joblib.load('model_client_3.pkl'),\n",
    "    4: joblib.load('model_client_4.pkl')\n",
    "}\n",
    "\n",
    "global_coef, global_intercept = joblib.load('global_model.pkl')\n",
    "\n",
    "# Evaluate with processed data\n",
    "results = evaluate_models_with_processed_data(X_processed, y_test, client_models, global_coef, global_intercept)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nModel Evaluation Results:\")\n",
    "print(\"-\" * 50)\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"RMSE: {metrics['rmse']:.4f}\")\n",
    "    print(f\"MSE: {metrics['mse']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "22534649-ac15-401b-af09-be326a38cb4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features after preprocessing: ['Soil_Type', 'Crop', 'Rainfall_mm', 'Temperature_Celsius', 'Fertilizer_Used', 'Irrigation_Used', 'Weather_Condition', 'Days_to_Harvest']\n",
      "Number of features after preprocessing: 8\n",
      "Processed test data shape: (200000, 8)\n",
      "Error evaluating Client_1: X has 8 features, but LinearRegression is expecting 17 features as input.\n",
      "Error evaluating Client_2: X has 8 features, but LinearRegression is expecting 17 features as input.\n",
      "\n",
      "Model Evaluation Results:\n",
      "--------------------------------------------------\n",
      "\n",
      "Client_3:\n",
      "RMSE: 0.2952\n",
      "MSE: 0.0871\n",
      "R²: 0.9130\n",
      "\n",
      "Client_4:\n",
      "RMSE: 0.2952\n",
      "MSE: 0.0871\n",
      "R²: 0.9130\n",
      "\n",
      "Global:\n",
      "RMSE: 0.2952\n",
      "MSE: 0.0871\n",
      "R²: 0.9130\n",
      "\n",
      "Best performing model: Client_4\n",
      "Best RMSE: 0.2952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SATYAJIT\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:347: InconsistentVersionWarning: Trying to unpickle estimator LinearRegression from version 1.3.2 when using version 1.3.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\SATYAJIT\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:457: UserWarning: X has feature names, but LinearRegression was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\SATYAJIT\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:457: UserWarning: X has feature names, but LinearRegression was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def preprocess_data_simple(df):\n",
    "    \"\"\"\n",
    "    Preprocess the data keeping original feature names without one-hot encoding\n",
    "    \"\"\"\n",
    "    # Make a copy to avoid modifying original data\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Initialize label encoders for categorical variables\n",
    "    categorical_features = ['Soil_Type', 'Crop', 'Weather_Condition']\n",
    "    label_encoders = {}\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    for feature in categorical_features:\n",
    "        label_encoders[feature] = LabelEncoder()\n",
    "        df[feature] = label_encoders[feature].fit_transform(df[feature])\n",
    "    \n",
    "    # Ensure all features are numeric\n",
    "    for col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col])\n",
    "    \n",
    "    # Get the feature order to match the original training data\n",
    "    feature_order = [\n",
    "        'Soil_Type',\n",
    "        'Crop',\n",
    "        'Rainfall_mm',\n",
    "        'Temperature_Celsius',\n",
    "        'Fertilizer_Used',\n",
    "        'Irrigation_Used',\n",
    "        'Weather_Condition',\n",
    "        'Days_to_Harvest'\n",
    "    ]\n",
    "    \n",
    "    return df[feature_order]\n",
    "\n",
    "# Preprocess test data\n",
    "X_processed = preprocess_data_simple(X_test)\n",
    "\n",
    "print(\"Features after preprocessing:\", list(X_processed.columns))\n",
    "print(\"Number of features after preprocessing:\", len(X_processed.columns))\n",
    "print(\"Processed test data shape:\", X_processed.shape)\n",
    "\n",
    "# Load and evaluate models\n",
    "def evaluate_models(X_processed, y_test, client_models, global_coef, global_intercept):\n",
    "    results = {}\n",
    "    \n",
    "    # Evaluate each client model\n",
    "    for client_id, model in client_models.items():\n",
    "        try:\n",
    "            # Get predictions\n",
    "            y_pred = model.predict(X_processed)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            mse = mean_squared_error(y_test, y_pred)\n",
    "            rmse = np.sqrt(mse)\n",
    "            \n",
    "            # Calculate R-squared\n",
    "            ss_tot = np.sum((y_test - np.mean(y_test)) ** 2)\n",
    "            ss_res = np.sum((y_test - y_pred) ** 2)\n",
    "            r2 = 1 - (ss_res / ss_tot)\n",
    "            \n",
    "            results[f'Client_{client_id}'] = {\n",
    "                'rmse': rmse,\n",
    "                'mse': mse,\n",
    "                'r2': r2\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating Client_{client_id}: {str(e)}\")\n",
    "    \n",
    "    # Evaluate global model\n",
    "    try:\n",
    "        global_pred = np.dot(X_processed, global_coef) + global_intercept\n",
    "        global_mse = mean_squared_error(y_test, global_pred)\n",
    "        global_rmse = np.sqrt(global_mse)\n",
    "        \n",
    "        # Calculate R-squared for global model\n",
    "        ss_tot = np.sum((y_test - np.mean(y_test)) ** 2)\n",
    "        ss_res = np.sum((y_test - global_pred) ** 2)\n",
    "        global_r2 = 1 - (ss_res / ss_tot)\n",
    "        \n",
    "        results['Global'] = {\n",
    "            'rmse': global_rmse,\n",
    "            'mse': global_mse,\n",
    "            'r2': global_r2\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating Global model: {str(e)}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Load models\n",
    "client_models = {\n",
    "    1: joblib.load('model_client_1.pkl'),\n",
    "    2: joblib.load('model_client_2.pkl'),\n",
    "    3: joblib.load('model_client_3.pkl'),\n",
    "    4: joblib.load('model_client_4.pkl')\n",
    "}\n",
    "\n",
    "global_coef, global_intercept = joblib.load('global_model.pkl')\n",
    "\n",
    "# Evaluate models\n",
    "results = evaluate_models(X_processed, y_test, client_models, global_coef, global_intercept)\n",
    "\n",
    "# Print results with additional details\n",
    "print(\"\\nModel Evaluation Results:\")\n",
    "print(\"-\" * 50)\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"RMSE: {metrics['rmse']:.4f}\")\n",
    "    print(f\"MSE: {metrics['mse']:.4f}\")\n",
    "    print(f\"R²: {metrics['r2']:.4f}\")\n",
    "\n",
    "# Find best performing model\n",
    "best_model = min(results.items(), key=lambda x: x[1]['rmse'])\n",
    "print(f\"\\nBest performing model: {best_model[0]}\")\n",
    "print(f\"Best RMSE: {best_model[1]['rmse']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "359beaf3-5e22-4d5d-bcd3-97465a1410b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distributing global model to clients...\n",
      "--------------------------------------------------\n",
      "Client 1: ✓\n",
      "  Model: distributed_models\\client_1\\global_model_v20241222_163858.pkl\n",
      "  Metadata: distributed_models\\client_1\\metadata_v20241222_163858.json\n",
      "Client 2: ✓\n",
      "  Model: distributed_models\\client_2\\global_model_v20241222_163858.pkl\n",
      "  Metadata: distributed_models\\client_2\\metadata_v20241222_163858.json\n",
      "Client 3: ✓\n",
      "  Model: distributed_models\\client_3\\global_model_v20241222_163858.pkl\n",
      "  Metadata: distributed_models\\client_3\\metadata_v20241222_163858.json\n",
      "Client 4: ✓\n",
      "  Model: distributed_models\\client_4\\global_model_v20241222_163858.pkl\n",
      "  Metadata: distributed_models\\client_4\\metadata_v20241222_163858.json\n",
      "\n",
      "Verifying distributions...\n",
      "--------------------------------------------------\n",
      "Client 1: ✓\n",
      "Client 2: ✓\n",
      "Client 3: ✓\n",
      "Client 4: ✓\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "\n",
    "class ModelDistributor:\n",
    "    def __init__(self, global_model_path='global_model.pkl', version=None):\n",
    "        self.global_model_path = global_model_path\n",
    "        self.version = version or datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        self.distribution_dir = Path('distributed_models')\n",
    "        self.distribution_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "    def calculate_model_hash(self, model_data):\n",
    "        \"\"\"Calculate SHA-256 hash of model data for integrity verification\"\"\"\n",
    "        return hashlib.sha256(str(model_data).encode()).hexdigest()\n",
    "    \n",
    "    def create_model_metadata(self, client_id, model_hash):\n",
    "        \"\"\"Create metadata for the distributed model\"\"\"\n",
    "        return {\n",
    "            'client_id': client_id,\n",
    "            'distribution_date': datetime.now().isoformat(),\n",
    "            'model_version': self.version,\n",
    "            'model_hash': model_hash,\n",
    "            'original_model': self.global_model_path\n",
    "        }\n",
    "    \n",
    "    def distribute_to_client(self, client_id):\n",
    "        \"\"\"Distribute global model to a specific client\"\"\"\n",
    "        try:\n",
    "            # Load global model\n",
    "            global_coef, global_intercept = joblib.load(self.global_model_path)\n",
    "            \n",
    "            # Create client-specific model directory\n",
    "            client_dir = self.distribution_dir / f'client_{client_id}'\n",
    "            client_dir.mkdir(exist_ok=True)\n",
    "            \n",
    "            # Create model file name with version\n",
    "            model_filename = f'global_model_v{self.version}.pkl'\n",
    "            model_path = client_dir / model_filename\n",
    "            \n",
    "            # Save model for client\n",
    "            model_data = (global_coef, global_intercept)\n",
    "            joblib.dump(model_data, model_path)\n",
    "            \n",
    "            # Calculate model hash\n",
    "            model_hash = self.calculate_model_hash(model_data)\n",
    "            \n",
    "            # Create and save metadata\n",
    "            metadata = self.create_model_metadata(client_id, model_hash)\n",
    "            metadata_path = client_dir / f'metadata_v{self.version}.json'\n",
    "            \n",
    "            with open(metadata_path, 'w') as f:\n",
    "                json.dump(metadata, f, indent=4)\n",
    "            \n",
    "            return {\n",
    "                'status': 'success',\n",
    "                'client_id': client_id,\n",
    "                'model_path': str(model_path),\n",
    "                'metadata_path': str(metadata_path)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'status': 'error',\n",
    "                'client_id': client_id,\n",
    "                'error': str(e)\n",
    "            }\n",
    "    \n",
    "    def distribute_to_all_clients(self, client_ids):\n",
    "        \"\"\"Distribute global model to all clients\"\"\"\n",
    "        results = []\n",
    "        for client_id in client_ids:\n",
    "            result = self.distribute_to_client(client_id)\n",
    "            results.append(result)\n",
    "            \n",
    "            # Print status message\n",
    "            status = \"✓\" if result['status'] == 'success' else \"✗\"\n",
    "            print(f\"Client {client_id}: {status}\")\n",
    "            \n",
    "            if result['status'] == 'error':\n",
    "                print(f\"  Error: {result['error']}\")\n",
    "            else:\n",
    "                print(f\"  Model: {result['model_path']}\")\n",
    "                print(f\"  Metadata: {result['metadata_path']}\")\n",
    "                \n",
    "        return results\n",
    "    \n",
    "    def verify_distribution(self, client_id):\n",
    "        \"\"\"Verify the integrity of a distributed model\"\"\"\n",
    "        try:\n",
    "            client_dir = self.distribution_dir / f'client_{client_id}'\n",
    "            model_path = client_dir / f'global_model_v{self.version}.pkl'\n",
    "            metadata_path = client_dir / f'metadata_v{self.version}.json'\n",
    "            \n",
    "            # Load distributed model and calculate hash\n",
    "            distributed_model = joblib.load(model_path)\n",
    "            current_hash = self.calculate_model_hash(distributed_model)\n",
    "            \n",
    "            # Load metadata and get stored hash\n",
    "            with open(metadata_path, 'r') as f:\n",
    "                metadata = json.load(f)\n",
    "            stored_hash = metadata['model_hash']\n",
    "            \n",
    "            # Compare hashes\n",
    "            return {\n",
    "                'client_id': client_id,\n",
    "                'verified': current_hash == stored_hash,\n",
    "                'model_version': metadata['model_version'],\n",
    "                'distribution_date': metadata['distribution_date']\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'client_id': client_id,\n",
    "                'verified': False,\n",
    "                'error': str(e)\n",
    "            }\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize distributor\n",
    "    distributor = ModelDistributor(global_model_path='global_model.pkl')\n",
    "    \n",
    "    # List of client IDs\n",
    "    client_ids = [1, 2, 3, 4]\n",
    "    \n",
    "    print(\"Distributing global model to clients...\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Distribute models\n",
    "    results = distributor.distribute_to_all_clients(client_ids)\n",
    "    \n",
    "    print(\"\\nVerifying distributions...\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Verify distributions\n",
    "    for client_id in client_ids:\n",
    "        verification = distributor.verify_distribution(client_id)\n",
    "        status = \"✓\" if verification.get('verified') else \"✗\"\n",
    "        print(f\"Client {client_id}: {status}\")\n",
    "        if not verification.get('verified'):\n",
    "            print(f\"  Error: {verification.get('error', 'Verification failed')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47bc27a1-5aba-48c5-b74b-b1deab90f526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 1 Coefficients: [ 1.29646234e+00  1.43479168e-01  7.49492860e-01  5.99692963e-01\n",
      "  1.20276851e-03  2.19722231e-03 -7.63652704e-04 -1.11523257e-03\n",
      "  1.09437005e-03  1.98662205e-03 -2.13774932e-03 -4.83673753e-03\n",
      "  2.75819816e-03 -6.03955228e-03 -4.41903616e-03 -3.82805169e-04\n",
      "  1.72747181e-03]\n",
      "Client 1 Intercept: 4.649473449370341\n",
      "Client 2 Coefficients: [ 1.29855473e+00  1.43170765e-01  7.51797574e-01  6.00038871e-01\n",
      "  9.00648904e-04  2.16979625e-03 -4.38318816e-03 -7.64832536e-05\n",
      "  4.68796958e-04 -6.50306727e-03  4.78925832e-03  3.99148694e-03\n",
      "  2.06905597e-03  3.06729233e-03  5.09248100e-03 -2.31231230e-04\n",
      "  5.65013950e-04]\n",
      "Client 2 Intercept: 4.642552360196861\n",
      "Client 3 Coefficients: [ 1.54702547e-04 -1.22302337e-03  7.64345295e-01  8.48115199e-02\n",
      "  4.41670814e-01  3.53623120e-01  5.37683894e-04  5.90079238e-05]\n",
      "Client 3 Intercept: 5.774058109752646e-05\n",
      "Client 4 Coefficients: [ 3.53241478e-04 -8.53621509e-04  7.66118385e-01  8.47888754e-02\n",
      "  4.41010276e-01  3.52858297e-01 -4.85743316e-04  1.15542825e-03]\n",
      "Client 4 Intercept: 0.0005938679868076994\n",
      "Model parameters saved successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SATYAJIT\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:347: InconsistentVersionWarning: Trying to unpickle estimator LinearRegression from version 1.3.2 when using version 1.3.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "# Load the models\n",
    "model_client_1 = joblib.load('model_client_1.pkl')\n",
    "model_client_2 = joblib.load('model_client_2.pkl')\n",
    "model_client_3 = joblib.load('model_client_3.pkl')\n",
    "model_client_4 = joblib.load('model_client_4.pkl')\n",
    "\n",
    "# Extract weights (coefficients)\n",
    "coef_1 = model_client_1.coef_\n",
    "coef_2 = model_client_2.coef_\n",
    "coef_3 = model_client_3.coef_\n",
    "coef_4 = model_client_4.coef_\n",
    "\n",
    "# Extract biases (intercepts)\n",
    "intercept_1 = model_client_1.intercept_\n",
    "intercept_2 = model_client_2.intercept_\n",
    "intercept_3 = model_client_3.intercept_\n",
    "intercept_4 = model_client_4.intercept_\n",
    "\n",
    "# Display results\n",
    "print(\"Client 1 Coefficients:\", coef_1)\n",
    "print(\"Client 1 Intercept:\", intercept_1)\n",
    "\n",
    "print(\"Client 2 Coefficients:\", coef_2)\n",
    "print(\"Client 2 Intercept:\", intercept_2)\n",
    "\n",
    "print(\"Client 3 Coefficients:\", coef_3)\n",
    "print(\"Client 3 Intercept:\", intercept_3)\n",
    "\n",
    "print(\"Client 4 Coefficients:\", coef_4)\n",
    "print(\"Client 4 Intercept:\", intercept_4)\n",
    "\n",
    "# Save coefficients and intercepts as .npy files\n",
    "np.save('coef_client_1.npy', coef_1)\n",
    "np.save('coef_client_2.npy', coef_2)\n",
    "np.save('coef_client_3.npy', coef_3)\n",
    "np.save('coef_client_4.npy', coef_4)\n",
    "\n",
    "np.save('intercept_client_1.npy', intercept_1)\n",
    "np.save('intercept_client_2.npy', intercept_2)\n",
    "np.save('intercept_client_3.npy', intercept_3)\n",
    "np.save('intercept_client_4.npy', intercept_4)\n",
    "\n",
    "print(\"Model parameters saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1198f30c-c1d3-4270-abca-7a27479a3060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<git.remote.PushInfo at 0x2a9e5929800>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import git\n",
    "import os\n",
    "\n",
    "# Initialize the repo and add remote\n",
    "repo_dir = r\"C:\\Users\\SATYAJIT\\crop yield\"  # Replace with the actual path\n",
    "repo = git.Repo.init(repo_dir)\n",
    "\n",
    "# Add remote (GitHub repository URL)\n",
    "origin = repo.create_remote('origin', 'https://github.com/555-Satyajit/Farm-params')  # Replace with your repo URL\n",
    "\n",
    "# Add files\n",
    "files_to_add = ['coef_client_1.npy', 'coef_client_2.npy', 'coef_client_3.npy', 'coef_client_4.npy',\n",
    "                'intercept_client_1.npy', 'intercept_client_2.npy', 'intercept_client_3.npy', 'intercept_client_4.npy']\n",
    "repo.index.add(files_to_add)\n",
    "\n",
    "# Commit the changes\n",
    "repo.index.commit(\"Upload model parameters\")\n",
    "\n",
    "# Push to GitHub\n",
    "origin.push('master')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f54f84d-ac8d-480f-9b42-366ae2998134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymongo\n",
      "  Obtaining dependency information for pymongo from https://files.pythonhosted.org/packages/02/68/b71c4106d03eef2482eade440c6f5737c2a4a42f6155726009f80ea38d06/pymongo-4.10.1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading pymongo-4.10.1-cp311-cp311-win_amd64.whl.metadata (22 kB)\n",
      "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo)\n",
      "  Obtaining dependency information for dnspython<3.0.0,>=1.16.0 from https://files.pythonhosted.org/packages/68/1b/e0a87d256e40e8c888847551b20a017a6b98139178505dc7ffb96f04e954/dnspython-2.7.0-py3-none-any.whl.metadata\n",
      "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Downloading pymongo-4.10.1-cp311-cp311-win_amd64.whl (876 kB)\n",
      "   ---------------------------------------- 0.0/876.5 kB ? eta -:--:--\n",
      "    --------------------------------------- 20.5/876.5 kB ? eta -:--:--\n",
      "   --- ------------------------------------ 81.9/876.5 kB 1.5 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 204.8/876.5 kB 2.1 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 348.2/876.5 kB 2.2 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 460.8/876.5 kB 2.4 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 522.2/876.5 kB 2.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 809.0/876.5 kB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 876.5/876.5 kB 2.9 MB/s eta 0:00:00\n",
      "Downloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
      "   ---------------------------------------- 0.0/313.6 kB ? eta -:--:--\n",
      "   ----------------------------------- ---- 276.5/313.6 kB 8.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 313.6/313.6 kB 6.5 MB/s eta 0:00:00\n",
      "Installing collected packages: dnspython, pymongo\n",
      "Successfully installed dnspython-2.7.0 pymongo-4.10.1\n"
     ]
    }
   ],
   "source": [
    "! pip install pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c1f682e-797c-4798-9fa4-b0ae481e9c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning the repository...\n",
      "Repository cloned successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import git\n",
    "\n",
    "# Define the repository URL and the local directory to clone into\n",
    "repo_url = \"https://github.com/555-Satyajit/Farm-params.git\"\n",
    "local_dir = r\"C:\\Users\\SATYAJIT\\crop yield\\Farm-Params\"  # The directory where the repo will be cloned\n",
    "\n",
    "# Clone the repository if it does not already exist\n",
    "if not os.path.exists(local_dir):\n",
    "    try:\n",
    "        print(\"Cloning the repository...\")\n",
    "        git.Repo.clone_from(repo_url, local_dir)\n",
    "        print(\"Repository cloned successfully!\")\n",
    "    except git.exc.GitCommandError as e:\n",
    "        print(f\"Error during cloning: {e}\")\n",
    "else:\n",
    "    print(\"Repository already exists, skipping clone.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42397ec1-c98c-4298-84bf-dcb30a02bd3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters for Client 1 saved to MongoDB!\n",
      "Model parameters for Client 2 saved to MongoDB!\n",
      "Model parameters for Client 3 saved to MongoDB!\n",
      "Model parameters for Client 4 saved to MongoDB!\n",
      "PermissionError: [WinError 5] Access is denied: 'Farm-params\\\\.git\\\\objects\\\\pack\\\\pack-9793a28fd66f88728bbd621fe2ee08d7dd711de3.idx'. Try closing any processes that may be using the folder.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Path to the cloned repository\n",
    "repo_path = \"Farm-params\"  # Adjust if your repo path is different\n",
    "\n",
    "# Initialize MongoDB client\n",
    "client = MongoClient('mongodb://localhost:27017/')  # Adjust if necessary\n",
    "db = client['fed_avg']  # Name of the database\n",
    "collection = db['model_params']  # Collection for storing model parameters\n",
    "\n",
    "# Loop over client numbers (1 to 4) to load parameters and store them in MongoDB\n",
    "for client_num in range(1, 5):\n",
    "    # Load the coefficients and intercepts for each client\n",
    "    coef = np.load(os.path.join(repo_path, f'coef_client_{client_num}.npy'))\n",
    "    intercept = np.load(os.path.join(repo_path, f'intercept_client_{client_num}.npy'))\n",
    "    \n",
    "    # Convert numpy arrays to lists for MongoDB compatibility\n",
    "    params = {\n",
    "        'coef': coef.tolist(),\n",
    "        'intercept': intercept.tolist()\n",
    "    }\n",
    "    \n",
    "    # Insert the data into MongoDB\n",
    "    collection.insert_one({f\"client_{client_num}_params\": params})\n",
    "    \n",
    "    print(f\"Model parameters for Client {client_num} saved to MongoDB!\")\n",
    "\n",
    "# Wait for any ongoing processes to release the folder before deletion\n",
    "time.sleep(2)\n",
    "\n",
    "# Try to delete the repository folder after processing\n",
    "try:\n",
    "    shutil.rmtree(repo_path)\n",
    "    print(f\"Successfully deleted the repository at {repo_path}.\")\n",
    "except PermissionError as e:\n",
    "    print(f\"PermissionError: {e}. Try closing any processes that may be using the folder.\")\n",
    "    # Optionally, you could try deleting the folder manually if needed.\n",
    "except Exception as e:\n",
    "    print(f\"Error during deletion: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "164d0de2-d25c-499f-8ac5-9beb869c064c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of coef_client_1: (17,)\n",
      "Shape of intercept_client_1: ()\n",
      "Model parameters for Client 1 saved to MongoDB!\n",
      "Shape of coef_client_2: (17,)\n",
      "Shape of intercept_client_2: ()\n",
      "Model parameters for Client 2 saved to MongoDB!\n",
      "Shape of coef_client_3: (8,)\n",
      "Shape of intercept_client_3: ()\n",
      "Model parameters for Client 3 saved to MongoDB!\n",
      "Shape of coef_client_4: (8,)\n",
      "Shape of intercept_client_4: ()\n",
      "Model parameters for Client 4 saved to MongoDB!\n",
      "Federated Averaging completed and global model saved to MongoDB!\n",
      "Global model saved to files!\n",
      "PermissionError: [WinError 5] Access is denied: 'Farm-params\\\\.git\\\\objects\\\\pack\\\\pack-9793a28fd66f88728bbd621fe2ee08d7dd711de3.idx'. Try closing any processes that may be using the folder.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from pymongo import MongoClient\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "# Path to the cloned repository\n",
    "repo_path = \"Farm-params\"  # Adjust if your repo path is different\n",
    "\n",
    "# Initialize MongoDB client\n",
    "client = MongoClient('mongodb://localhost:27017/')  # Adjust if necessary\n",
    "db = client['fed_avg']  # Name of the database\n",
    "collection = db['model_params']  # Collection for storing model parameters\n",
    "\n",
    "# Arrays to hold the coefficients and intercepts for all clients\n",
    "coef_array = []\n",
    "intercept_array = []\n",
    "\n",
    "# Target size (max size) for coefficients (to ensure uniform shape)\n",
    "max_coef_size = 17  # From coef_client_1 and coef_client_2\n",
    "\n",
    "# Loop over client numbers (1 to 4) to load parameters and store them in MongoDB\n",
    "for client_num in range(1, 5):\n",
    "    # Load the coefficients and intercepts for each client\n",
    "    coef = np.load(os.path.join(repo_path, f'coef_client_{client_num}.npy'))\n",
    "    intercept = np.load(os.path.join(repo_path, f'intercept_client_{client_num}.npy'))\n",
    "    \n",
    "    # Print the shapes of the arrays to check for consistency\n",
    "    print(f\"Shape of coef_client_{client_num}: {coef.shape}\")\n",
    "    print(f\"Shape of intercept_client_{client_num}: {intercept.shape}\")\n",
    "    \n",
    "    # Pad smaller arrays with zeros to match the target size (max_coef_size)\n",
    "    if coef.shape[0] < max_coef_size:\n",
    "        coef = np.pad(coef, (0, max_coef_size - coef.shape[0]), mode='constant', constant_values=0)\n",
    "    \n",
    "    # Store the coefficients and intercepts in arrays for later aggregation\n",
    "    coef_array.append(coef)\n",
    "    intercept_array.append(intercept)\n",
    "    \n",
    "    # Convert numpy arrays to lists for MongoDB compatibility\n",
    "    params = {\n",
    "        'coef': coef.tolist(),\n",
    "        'intercept': intercept.tolist()\n",
    "    }\n",
    "    \n",
    "    # Insert the data into MongoDB\n",
    "    collection.insert_one({f\"client_{client_num}_params\": params})\n",
    "    print(f\"Model parameters for Client {client_num} saved to MongoDB!\")\n",
    "\n",
    "# Check if all arrays have the same shape before proceeding with aggregation\n",
    "# Now that all arrays have the same length, perform Federated Averaging (FedAvg)\n",
    "try:\n",
    "    # Perform Federated Averaging (FedAvg)\n",
    "    avg_coef = np.mean(coef_array, axis=0)\n",
    "    avg_intercept = np.mean(intercept_array, axis=0)\n",
    "\n",
    "    # Save the aggregated parameters (FedAvg result) as the global model\n",
    "    aggregation_result = {\n",
    "        'coef': avg_coef.tolist(),\n",
    "        'intercept': avg_intercept.tolist()\n",
    "    }\n",
    "\n",
    "    # Insert the global model into MongoDB\n",
    "    collection.insert_one({\"fed_avg_model_params\": aggregation_result})\n",
    "    print(\"Federated Averaging completed and global model saved to MongoDB!\")\n",
    "\n",
    "    # Optionally, save the global model to a file\n",
    "    np.save('global_model_coef.npy', avg_coef)\n",
    "    np.save('global_model_intercept.npy', avg_intercept)\n",
    "    print(\"Global model saved to files!\")\n",
    "\n",
    "except ValueError as e:\n",
    "    print(f\"Error during aggregation: {e}. Check if the parameter shapes are consistent across clients.\")\n",
    "\n",
    "# Optionally, delete the cloned repo after processing\n",
    "repo_path = \"Farm-params\"\n",
    "\n",
    "# Wait for any ongoing processes to release the folder\n",
    "time.sleep(2)\n",
    "\n",
    "# Try to delete the repository folder\n",
    "try:\n",
    "    shutil.rmtree(repo_path)\n",
    "    print(f\"Successfully deleted the repository at {repo_path}.\")\n",
    "except PermissionError as e:\n",
    "    print(f\"PermissionError: {e}. Try closing any processes that may be using the folder.\")\n",
    "    # Optionally, you could try deleting the folder manually if needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce25a2f7-488b-4f4c-b0ec-d58df4056e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "! streamlit run app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94b0ce12-50fb-4f4f-927b-0b7f5d7a2e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository already exists, skipping clone.\n",
      "Shape of coef_client_1: (17,)\n",
      "Shape of intercept_client_1: ()\n",
      "Shape of coef_client_2: (17,)\n",
      "Shape of intercept_client_2: ()\n",
      "Shape of coef_client_3: (8,)\n",
      "Shape of intercept_client_3: ()\n",
      "Shape of coef_client_4: (8,)\n",
      "Shape of intercept_client_4: ()\n",
      "Global model saved as globalmodel1.pkl!\n",
      "PermissionError: [WinError 5] Access is denied: 'Farm-params\\\\.git\\\\objects\\\\pack\\\\pack-9793a28fd66f88728bbd621fe2ee08d7dd711de3.idx'. Try closing any processes that may be using the folder.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import git\n",
    "import numpy as np\n",
    "import shutil\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "def main():\n",
    "    # Define the repository URL and the local directory to clone into\n",
    "    repo_url = \"https://github.com/555-Satyajit/Farm-params.git\"\n",
    "    local_dir = r\"C:\\Users\\SATYAJIT\\crop yield\\Farm-Params\"\n",
    "    repo_path = \"Farm-params\"\n",
    "\n",
    "    # Clone the repository if it does not already exist\n",
    "    if not os.path.exists(local_dir):\n",
    "        try:\n",
    "            print(\"Cloning the repository...\")\n",
    "            git.Repo.clone_from(repo_url, local_dir)\n",
    "            print(\"Repository cloned successfully!\")\n",
    "        except git.exc.GitCommandError as e:\n",
    "            print(f\"Error during cloning: {e}\")\n",
    "            return\n",
    "    else:\n",
    "        print(\"Repository already exists, skipping clone.\")\n",
    "\n",
    "    # Arrays to hold the coefficients and intercepts for all clients\n",
    "    coef_array = []\n",
    "    intercept_array = []\n",
    "    max_coef_size = 17  # From coef_client_1 and coef_client_2\n",
    "\n",
    "    # Load and process parameters from each client\n",
    "    for client_num in range(1, 5):\n",
    "        try:\n",
    "            # Load the coefficients and intercepts for each client\n",
    "            coef = np.load(os.path.join(repo_path, f'coef_client_{client_num}.npy'))\n",
    "            intercept = np.load(os.path.join(repo_path, f'intercept_client_{client_num}.npy'))\n",
    "            \n",
    "            print(f\"Shape of coef_client_{client_num}: {coef.shape}\")\n",
    "            print(f\"Shape of intercept_client_{client_num}: {intercept.shape}\")\n",
    "            \n",
    "            # Pad smaller arrays with zeros to match the target size\n",
    "            if coef.shape[0] < max_coef_size:\n",
    "                coef = np.pad(coef, (0, max_coef_size - coef.shape[0]), \n",
    "                            mode='constant', constant_values=0)\n",
    "            \n",
    "            # Store the coefficients and intercepts in arrays\n",
    "            coef_array.append(coef)\n",
    "            intercept_array.append(intercept)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing client {client_num}: {e}\")\n",
    "            continue\n",
    "\n",
    "    try:\n",
    "        # Perform Federated Averaging (FedAvg)\n",
    "        avg_coef = np.mean(coef_array, axis=0)\n",
    "        avg_intercept = np.mean(intercept_array, axis=0)\n",
    "\n",
    "        # Create a dictionary to store the global model parameters\n",
    "        global_model = {\n",
    "            'coefficients': avg_coef,\n",
    "            'intercept': avg_intercept\n",
    "        }\n",
    "\n",
    "        # Save the global model using pickle\n",
    "        with open('globalmodel1.pkl', 'wb') as f:\n",
    "            pickle.dump(global_model, f)\n",
    "        print(\"Global model saved as globalmodel1.pkl!\")\n",
    "\n",
    "    except ValueError as e:\n",
    "        print(f\"Error during aggregation: {e}. Check if the parameter shapes are consistent across clients.\")\n",
    "\n",
    "    # Clean up: Delete the repository\n",
    "    time.sleep(2)  # Wait for any ongoing processes\n",
    "    try:\n",
    "        shutil.rmtree(repo_path)\n",
    "        print(f\"Successfully deleted the repository at {repo_path}.\")\n",
    "    except PermissionError as e:\n",
    "        print(f\"PermissionError: {e}. Try closing any processes that may be using the folder.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during deletion: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "606a23fd-c7a0-4bdc-b9cd-c88a5c4ee2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository already exists, skipping clone.\n",
      "\n",
      "Federated Learning Round 1\n",
      "Selected clients: [1, 3, 4]\n",
      "Client 1 completed local training\n",
      "Client 3 completed local training\n",
      "Client 4 completed local training\n",
      "Round 1 aggregation completed\n",
      "\n",
      "Federated Learning Round 2\n",
      "Selected clients: [1, 3, 4]\n",
      "Client 1 completed local training\n",
      "Client 3 completed local training\n",
      "Client 4 completed local training\n",
      "Round 2 aggregation completed\n",
      "\n",
      "Federated Learning Round 3\n",
      "Selected clients: [2, 3, 1]\n",
      "Client 2 completed local training\n",
      "Client 3 completed local training\n",
      "Client 1 completed local training\n",
      "Round 3 aggregation completed\n",
      "\n",
      "Federated Learning Round 4\n",
      "Selected clients: [4, 2, 1]\n",
      "Client 4 completed local training\n",
      "Client 2 completed local training\n",
      "Client 1 completed local training\n",
      "Round 4 aggregation completed\n",
      "\n",
      "Federated Learning Round 5\n",
      "Selected clients: [1, 4, 2]\n",
      "Client 1 completed local training\n",
      "Client 4 completed local training\n",
      "Client 2 completed local training\n",
      "Round 5 aggregation completed\n",
      "\n",
      "Final global model saved as globalmodel1.pkl!\n",
      "Error during cleanup: [WinError 5] Access is denied: 'Farm-params\\\\.git\\\\objects\\\\pack\\\\pack-9793a28fd66f88728bbd621fe2ee08d7dd711de3.idx'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import git\n",
    "import numpy as np\n",
    "import shutil\n",
    "import time\n",
    "import pickle\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from typing import Dict, List\n",
    "import random\n",
    "\n",
    "class FederatedServer:\n",
    "    def __init__(self, num_rounds: int = 5, client_fraction: float = 0.8):\n",
    "        self.global_model = None\n",
    "        self.num_rounds = num_rounds\n",
    "        self.client_fraction = client_fraction\n",
    "        self.clients = []\n",
    "        \n",
    "    def initialize_global_model(self):\n",
    "        \"\"\"Initialize global model parameters\"\"\"\n",
    "        self.global_model = {\n",
    "            'coefficients': None,\n",
    "            'intercept': None\n",
    "        }\n",
    "    \n",
    "    def select_clients(self) -> List[int]:\n",
    "        \"\"\"Randomly select a fraction of clients for each round\"\"\"\n",
    "        num_clients = len(self.clients)\n",
    "        num_selected = max(1, int(self.client_fraction * num_clients))\n",
    "        return random.sample(range(num_clients), num_selected)\n",
    "    \n",
    "    def aggregate_models(self, client_models: List[Dict]) -> Dict:\n",
    "        \"\"\"Aggregate client models using FedAvg\"\"\"\n",
    "        coef_array = []\n",
    "        intercept_array = []\n",
    "        \n",
    "        # Get the maximum coefficient size\n",
    "        max_coef_size = max(coef.shape[0] for coef in \n",
    "                          [model['coefficients'] for model in client_models])\n",
    "        \n",
    "        for model in client_models:\n",
    "            coef = model['coefficients']\n",
    "            # Pad smaller arrays with zeros\n",
    "            if coef.shape[0] < max_coef_size:\n",
    "                coef = np.pad(coef, (0, max_coef_size - coef.shape[0]), \n",
    "                            mode='constant', constant_values=0)\n",
    "            coef_array.append(coef)\n",
    "            intercept_array.append(model['intercept'])\n",
    "        \n",
    "        return {\n",
    "            'coefficients': np.mean(coef_array, axis=0),\n",
    "            'intercept': np.mean(intercept_array, axis=0)\n",
    "        }\n",
    "\n",
    "class FederatedClient:\n",
    "    def __init__(self, client_id: int, data_path: str):\n",
    "        self.client_id = client_id\n",
    "        self.data_path = data_path\n",
    "        self.model = LinearRegression()\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "    def load_data(self):\n",
    "        \"\"\"Load client's local data\"\"\"\n",
    "        # In your case, loading pre-trained coefficients and intercepts\n",
    "        self.coef = np.load(os.path.join(self.data_path, f'coef_client_{self.client_id}.npy'))\n",
    "        self.intercept = np.load(os.path.join(self.data_path, \n",
    "                                            f'intercept_client_{self.client_id}.npy'))\n",
    "    \n",
    "    def train_local_model(self, global_model=None):\n",
    "        \"\"\"Train local model using client's data\"\"\"\n",
    "        if global_model is not None and global_model['coefficients'] is not None:\n",
    "            # Initialize local model with global parameters\n",
    "            self.coef = global_model['coefficients']\n",
    "            self.intercept = global_model['intercept']\n",
    "        \n",
    "        # In a real implementation, you would train the model here\n",
    "        # For now, we're just using the pre-trained parameters\n",
    "        \n",
    "        return {\n",
    "            'coefficients': self.coef,\n",
    "            'intercept': self.intercept\n",
    "        }\n",
    "\n",
    "def main():\n",
    "    # Define paths\n",
    "    repo_url = \"https://github.com/555-Satyajit/Farm-params.git\"\n",
    "    local_dir = r\"C:\\Users\\SATYAJIT\\crop yield\\Farm-Params\"\n",
    "    repo_path = \"Farm-params\"\n",
    "\n",
    "    # Clone repository\n",
    "    if not os.path.exists(local_dir):\n",
    "        try:\n",
    "            print(\"Cloning the repository...\")\n",
    "            git.Repo.clone_from(repo_url, local_dir)\n",
    "            print(\"Repository cloned successfully!\")\n",
    "        except git.exc.GitCommandError as e:\n",
    "            print(f\"Error during cloning: {e}\")\n",
    "            return\n",
    "    else:\n",
    "        print(\"Repository already exists, skipping clone.\")\n",
    "\n",
    "    # Initialize server\n",
    "    server = FederatedServer(num_rounds=5, client_fraction=0.8)\n",
    "    server.initialize_global_model()\n",
    "\n",
    "    # Initialize clients\n",
    "    for client_id in range(1, 5):\n",
    "        client = FederatedClient(client_id, repo_path)\n",
    "        try:\n",
    "            client.load_data()\n",
    "            server.clients.append(client)\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing client {client_id}: {e}\")\n",
    "\n",
    "    # Federated Learning rounds\n",
    "    for round_num in range(server.num_rounds):\n",
    "        print(f\"\\nFederated Learning Round {round_num + 1}\")\n",
    "        \n",
    "        # Select clients for this round\n",
    "        selected_clients = server.select_clients()\n",
    "        print(f\"Selected clients: {[i+1 for i in selected_clients]}\")\n",
    "        \n",
    "        # Collect client updates\n",
    "        client_models = []\n",
    "        for client_idx in selected_clients:\n",
    "            client = server.clients[client_idx]\n",
    "            # Train local model (in this case, just using pre-trained parameters)\n",
    "            client_model = client.train_local_model(server.global_model)\n",
    "            client_models.append(client_model)\n",
    "            print(f\"Client {client.client_id} completed local training\")\n",
    "        \n",
    "        # Aggregate models\n",
    "        server.global_model = server.aggregate_models(client_models)\n",
    "        print(f\"Round {round_num + 1} aggregation completed\")\n",
    "\n",
    "    # Save final global model\n",
    "    try:\n",
    "        with open('globalmodel1.pkl', 'wb') as f:\n",
    "            pickle.dump(server.global_model, f)\n",
    "        print(\"\\nFinal global model saved as globalmodel1.pkl!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving global model: {e}\")\n",
    "\n",
    "    # Cleanup\n",
    "    time.sleep(2)\n",
    "    try:\n",
    "        shutil.rmtree(repo_path)\n",
    "        print(f\"Successfully deleted the repository at {repo_path}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during cleanup: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b0bde2-801a-4433-875b-d39e27441e2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
